{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "environment_rows = 5\n",
    "environment_cols = 5\n",
    "\n",
    "# Creating a 3D numpy array to hold current Q-values for each state and action pair: Q(s, a)\n",
    "# q_values = np.zeros((environment_rows, environment_cols, 4))\n",
    "\n",
    "import numpy as np\n",
    "class QTable:\n",
    "    def __init__(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_cols = environment_cols\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((environment_rows,environment_cols, num_actions))\n",
    "    \n",
    "    # Getting Q value at certain state and action\n",
    "    def get_q_value(self, environment_rows, environment_cols, action):\n",
    "        return self.q_table[environment_rows, environment_cols, action]\n",
    "    \n",
    "    # Updating Q table using the Bellman Equation\n",
    "    def update_q_value(self, environment_rows, environment_cols, action, reward, next_row, next_col):\n",
    "        current_q_value = self.q_table[environment_rows, environment_cols, action]\n",
    "        max_next_q_value = np.max(self.q_table[next_row, next_col])\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "        self.q_table[environment_rows, environment_cols, action] = new_q_value\n",
    "    \n",
    "    # Printing Q-table\n",
    "    def print_q_table(self):\n",
    "        print(\"Q-table:\")\n",
    "        print(\"State    |   N       E       S       W\")\n",
    "        print(\"----------------------------------------\")\n",
    "        for row in range(self.environment_rows):\n",
    "            for col in range(self.environment_cols):\n",
    "                print(f\"({row},{col})   | \", end=\"\")\n",
    "                for action in actions:  # Iterate over 'actions' instead of 'self.num_actions'\n",
    "                    action_idx = actions.index(action)\n",
    "                    q_value_str = f\"{self.q_table[row, col, action_idx]:.2f}\"  \n",
    "                    padding = max(0, 7 - len(q_value_str))  \n",
    "                    print(\" \" * (padding // 2) + q_value_str + \" \" * ((padding + 1) // 2), end=\"\")\n",
    "                print()\n",
    "\n",
    "\n",
    "# numeric action codes: 0 = N, 1 = E, 2  = S, 3 = W\n",
    "actions = ['N', 'E', 'S','W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 -1 -1 -1 -1]\n",
      "[ -1 -10  -1  -1  -1]\n",
      "[-10  -1  -1  -1  -1]\n",
      "[-1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Defining environment\n",
    "rewards = np.full((environment_rows, environment_cols), -1)\n",
    "\n",
    "rewards[0,0] = 10\n",
    "rewards[1,1] = -10\n",
    "rewards[2,0] = -10\n",
    "\n",
    "for row in rewards:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Q-values represent our current estimate of the sum of all future rewards if we were to take a particular action in a particular state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if specific location is a terminal state\n",
    "def is_terminal(cur_row_idx, cur_col_idx):\n",
    "    # if the reward is -1 then it is not a terminal state\n",
    "    if rewards[cur_row_idx, cur_col_idx] == -1: return False\n",
    "    else: return True\n",
    "\n",
    "# Epsilon greedy algorithm that will choose which acion to take next\n",
    "def get_next_action(q_values, cur_row_idx, cur_col_idx, epsilon):\n",
    "    # if a randomly chosen value between 0 and 1 is less than epsilon, choose most promising value from Q-table for this state [90%]\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values.q_table[cur_row_idx, cur_col_idx])\n",
    "    else: # choose random action for exploration\n",
    "        return np.random.randint(4)\n",
    "    \n",
    "# Get next location based on chosen action\n",
    "def get_next_location(cur_row_idx, cur_col_idx, action_idx):\n",
    "    new_row_idx = cur_row_idx\n",
    "    new_col_idx = cur_col_idx\n",
    "    if actions[action_idx] == 'N' and cur_row_idx > 0:\n",
    "        new_row_idx -= 1\n",
    "    elif actions[action_idx] == 'E' and cur_col_idx < environment_cols - 1:\n",
    "        new_col_idx += 1\n",
    "    elif actions[action_idx] == 'S' and cur_row_idx < environment_rows - 1:\n",
    "        new_row_idx += 1\n",
    "    elif actions[action_idx] == 'W' and cur_col_idx > 0:\n",
    "        new_col_idx -= 1\n",
    "    return new_row_idx, new_col_idx\n",
    "\n",
    "#Get the shortest path between any location within the space that the agent is allowed to travel and the item packaging location.\n",
    "def get_shortest_path(q_values, start_row_index, start_column_index):\n",
    "  #return immediately if this is an invalid starting location\n",
    "  if not is_terminal(start_row_index, start_column_index):  # Inverted the condition\n",
    "        return []\n",
    "  else: #if this is a 'legal' starting location\n",
    "      current_row_index, current_column_index = start_row_index, start_column_index\n",
    "      shortest_path = []\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "      #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "      while not is_terminal(current_row_index, current_column_index):\n",
    "          #get the best action to take\n",
    "          action_index = get_next_action(q_values, current_row_index, current_column_index, 1.)\n",
    "          #move to the next location on the path, and add the new location to the list\n",
    "          current_row_index, current_column_index = get_next_location(q_values, current_row_index, current_column_index, action_index)\n",
    "          shortest_path.append([current_row_index, current_column_index])\n",
    "      return shortest_path\n",
    "  \n",
    "  #define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "  current_row_index = np.random.randint(environment_rows)\n",
    "  current_column_index = np.random.randint(environment_cols)\n",
    "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
    "  #(i.e., until the chosen state is a 'white square').\n",
    "  while is_terminal(current_row_index, current_column_index):\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_cols)\n",
    "  return current_row_index, current_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "\n",
    "q_values = QTable(environment_rows,environment_cols, 4, learning_rate,discount_factor)\n",
    "\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "  #get the starting location for this episode\n",
    "  row_index, column_index = get_starting_location()\n",
    "\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "  while not is_terminal(row_index, column_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(q_values, row_index, column_index, epsilon)\n",
    "\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "    \n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    # old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    # temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    # new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    # q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "    q_values.update_q_value(old_row_index, old_column_index, action_index, reward, row_index, column_index)\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(get_shortest_path(q_values, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "State    |   N       E       S       W\n",
      "----------------------------------------\n",
      "(0,0)   |  0.00   0.00   0.00   0.00  \n",
      "(0,1)   |  8.00   6.20  -10.00  10.00 \n",
      "(0,2)   |  6.20   4.58   4.58   8.00  \n",
      "(0,3)   |  4.58   3.07   3.12   6.20  \n",
      "(0,4)   |  3.06   -3.14  1.31   4.58  \n",
      "(1,0)   |  10.00  0.00   -9.00  7.20  \n",
      "(1,1)   |  0.00   0.00   0.00   0.00  \n",
      "(1,2)   |  6.20   3.12   3.12  -10.00 \n",
      "(1,3)   |  4.52   1.38   1.44   4.58  \n",
      "(1,4)   |  -3.14  1.37   0.63   3.12  \n",
      "(2,0)   |  0.00   0.00   0.00   0.00  \n",
      "(2,1)   |  -9.99  3.12   0.63  -10.00 \n",
      "(2,2)   |  4.58   1.81   1.81   1.81  \n",
      "(2,3)   |  3.12   0.63   0.63   3.12  \n",
      "(2,4)   |  1.81   0.63   -0.43  1.81  \n",
      "(3,0)   |  -9.00  0.63   -1.39  -0.47 \n",
      "(3,1)   |  1.81   1.81   -0.43  -0.46 \n",
      "(3,2)   |  3.12   0.63   0.59   0.62  \n",
      "(3,3)   |  1.81   -0.47  -0.47  1.81  \n",
      "(3,4)   |  0.63   -0.47  -1.39  0.63  \n",
      "(4,0)   |  -0.43  -0.77  -1.42  -1.39 \n",
      "(4,1)   |  0.63   -3.77  -3.86  -1.42 \n",
      "(4,2)   |  1.81   -0.47  0.62   -0.77 \n",
      "(4,3)   |  0.63   -1.65  -0.82  0.18  \n",
      "(4,4)   |  -0.43  -1.68  -1.39  -4.31 \n"
     ]
    }
   ],
   "source": [
    "q_values.print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Q-table\n",
    "print(\"Q-table:\")\n",
    "print(\"State    |   N       E       S       W\")\n",
    "print(\"----------------------------------------\")\n",
    "for row in range(environment_rows):\n",
    "    for col in range(environment_cols):\n",
    "        print(f\"({row},{col})   | \", end=\"\")\n",
    "        for action_idx, action in enumerate(actions):\n",
    "            q_value_str = f\"{q_values[row, col, action_idx]:.2f}\"  # Convert Q-value to string\n",
    "            padding = max(0, 7 - len(q_value_str))  # Calculate padding to center-align\n",
    "            print(\" \" * (padding // 2) + q_value_str + \" \" * ((padding + 1) // 2), end=\"\")\n",
    "        print()  # Move to the next line for the next state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QTable:\n",
    "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        current_q_value = self.q_table[state, action]\n",
    "        max_next_q_value = np.max(self.q_table[next_state])\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + \\\n",
    "                      self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table[state, action]\n",
    "\n",
    "    def update_learning_rate(self, new_learning_rate):\n",
    "        self.learning_rate = new_learning_rate\n",
    "\n",
    "    def update_discount_factor(self, new_discount_factor):\n",
    "        self.discount_factor = new_discount_factor\n",
    "\n",
    "    def get_optimal_action(self, state, exploration_rate):\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.randint(self.num_actions)  # Randomly choose an action\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Choose action with maximum Q-value\n",
    "\n",
    "class BlockTransportationProblem:\n",
    "    def __init__(self, num_agents, num_states, num_actions, learning_rate, discount_factor):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_tables = [QTable(num_states, num_actions, learning_rate, discount_factor) for _ in range(num_agents)]\n",
    "\n",
    "    def update_q_values(self, agent_index, state, action, reward, next_state):\n",
    "        self.q_tables[agent_index].update_q_value(state, action, reward, next_state)\n",
    "\n",
    "    def select_action(self, agent_index, state, exploration_rate, policy):\n",
    "        if policy == \"PRANDOM\":\n",
    "            return np.random.randint(self.num_actions)  # Choose action randomly\n",
    "        elif policy == \"PEXPLOIT\":\n",
    "            return self.q_tables[agent_index].get_optimal_action(state, exploration_rate)\n",
    "        elif policy == \"PGREEDY\":\n",
    "            return np.argmax(self.q_tables[agent_index].q_table[state])  # Choose action with maximum Q-value\n",
    "\n",
    "# Experiment parameters\n",
    "num_agents = 3\n",
    "num_states = 25  # Assuming 25 states\n",
    "num_actions = 4  # Assuming 4 actions (north, south, east, west)\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.5\n",
    "total_steps = 9000\n",
    "initial_prandom_steps = 500\n",
    "remaining_steps = total_steps - initial_prandom_steps\n",
    "policy_switch_step = initial_prandom_steps\n",
    "\n",
    "# Initialize Block Transportation Problem\n",
    "btp = BlockTransportationProblem(num_agents, num_states, num_actions,learning_rate,discount_factor)\n",
    "\n",
    "# Run PRANDOM for initial steps\n",
    "for step in range(initial_prandom_steps):\n",
    "    states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    actions = [btp.select_action(agent_index, state, exploration_rate=1.0, policy=\"PRANDOM\") for agent_index, state in enumerate(states)]\n",
    "    rewards = [np.random.randint(10) for _ in range(num_agents)]  # Random rewards for demonstration\n",
    "    next_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    for agent_index in range(num_agents):\n",
    "        btp.update_q_values(agent_index, states[agent_index], actions[agent_index], rewards[agent_index], next_states[agent_index])\n",
    "\n",
    "# Switch policies and continue running\n",
    "for step in range(policy_switch_step, total_steps):\n",
    "    policy = \"PGREEDY\" if step < (total_steps - remaining_steps // 2) else \"PEXPLOIT\"\n",
    "    exploration_rate = 0.1 if policy == \"PEXPLOIT\" else 1.0\n",
    "    states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    actions = [btp.select_action(agent_index, state, exploration_rate, policy) for agent_index, state in enumerate(states)]\n",
    "    rewards = [np.random.randint(10) for _ in range(num_agents)]  # Random rewards for demonstration\n",
    "    next_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "    for agent_index in range(num_agents):\n",
    "        btp.update_q_values(agent_index, states[agent_index], actions[agent_index], rewards[agent_index], next_states[agent_index])\n",
    "\n",
    "final_states = [np.random.randint(num_states) for _ in range(num_agents)]\n",
    "\n",
    "policy = \"PEXPLOIT\"\n",
    "final_q_table = btp.q_tables[0].q_table  # Assuming agents have the same Q-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.01220137  6.13672674  6.98778214  7.01159269]\n",
      " [ 6.78404457  6.7250203  10.64233301  6.74363573]\n",
      " [ 7.33714983  7.04439873  8.31434864  7.3339964 ]\n",
      " [ 7.4553789   7.70668007  6.47870605  6.67849751]\n",
      " [ 6.04543445  7.70939295 11.0050168   7.55708811]\n",
      " [ 5.70530482  6.63762718  9.99809517  6.33906805]\n",
      " [ 6.89203772  7.76551332  6.08185066  6.69126486]\n",
      " [ 6.23086194  7.14428195  5.98611037  8.47341871]\n",
      " [ 8.52057004  7.32477045  5.00039369  7.7302224 ]\n",
      " [ 7.34850834  8.22849181  7.34694474  7.07894112]\n",
      " [10.14615783  6.04667622  6.98903834  6.71591524]\n",
      " [ 6.18634053  6.64962803  7.443523    6.77777683]\n",
      " [ 6.51301253  6.80196149  7.78394682  6.72176276]\n",
      " [ 7.14300627  6.72583059  7.27185197 10.22577046]\n",
      " [ 6.03654291  5.71617141  6.21732942 11.81857238]\n",
      " [ 5.98059047  6.12271529  6.617482    6.44049571]\n",
      " [ 7.75175476 11.50857413  6.66740874  6.93831157]\n",
      " [ 6.51035958  8.86283463  9.30986562  7.1339123 ]\n",
      " [ 5.66532844  6.28788866  8.26787125  9.98940664]\n",
      " [ 5.98188348  6.44886821  7.71965027  9.15314569]\n",
      " [ 7.88053579  6.8133878   6.101315    7.87569022]\n",
      " [ 6.92150537  8.97730671  6.81728238  6.86122721]\n",
      " [ 8.01941467  7.02288158  7.1025057   7.65332746]\n",
      " [ 5.30066792  6.47876025  8.56200532  6.28955418]\n",
      " [ 7.48217037  6.48642461  7.82965347  7.85519577]]\n",
      "[3, 6, 12]\n"
     ]
    }
   ],
   "source": [
    "print(final_q_table)\n",
    "print(final_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
