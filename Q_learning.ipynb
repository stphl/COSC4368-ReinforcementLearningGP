{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "environment_rows = 5\n",
    "environment_cols = 5\n",
    "\n",
    "# Creating a 3D numpy array to hold current Q-values for each state and action pair: Q(s, a)\n",
    "q_values = np.zeros((environment_rows, environment_cols, 4))\n",
    "\n",
    "# numeric action codes: 0 = N, 1 = E, 2  = S, 3 = W\n",
    "actions = ['N', 'E', 'S','W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 -1 -1 -1 -1]\n",
      "[ -1 -10  -1  -1  -1]\n",
      "[-10  -1  -1  -1  -1]\n",
      "[-1 -1 -1 -1 -1]\n",
      "[-1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Defining environment\n",
    "rewards = np.full((environment_rows, environment_cols), -1)\n",
    "\n",
    "rewards[0,0] = 10\n",
    "rewards[1,1] = -10\n",
    "rewards[2,0] = -10\n",
    "\n",
    "for row in rewards:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Q-values represent our current estimate of the sum of all future rewards if we were to take a particular action in a particular state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if specific location is a terminal state\n",
    "def is_terminal(cur_row_idx, cur_col_idx):\n",
    "    # if the reward is -1 then it is not a terminal state\n",
    "    if rewards[cur_row_idx, cur_col_idx] == -1: return False\n",
    "    else: return True\n",
    "\n",
    "# Epsilon greedy algorithm that will choose which acion to take next\n",
    "def get_next_action(cur_row_idx, cur_col_idx, epsilon):\n",
    "    # if a randomly chosen value between 0 and 1 is less than epsilon, choose most promising value from Q-table for this state [90%]\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[cur_row_idx, cur_col_idx])\n",
    "    else: # choose random action for exploration\n",
    "        return np.random.randint(4)\n",
    "    \n",
    "# Get next location based on chosen action\n",
    "def get_next_location(cur_row_idx, cur_col_idx, action_idx):\n",
    "    new_row_idx = cur_row_idx\n",
    "    new_col_idx = cur_col_idx\n",
    "    if actions[action_idx] == 'N' and cur_row_idx > 0:\n",
    "        new_row_idx -= 1\n",
    "    elif actions[action_idx] == 'E' and cur_col_idx < environment_cols - 1:\n",
    "        new_col_idx += 1\n",
    "    elif actions[action_idx] == 'S' and cur_row_idx < environment_rows - 1:\n",
    "        new_row_idx += 1\n",
    "    elif actions[action_idx] == 'W' and cur_col_idx > 0:\n",
    "        new_col_idx -= 1\n",
    "    return new_row_idx, new_col_idx\n",
    "\n",
    "#Get the shortest path between any location within the space that the agent is allowed to travel and the item packaging location.\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  #return immediately if this is an invalid starting location\n",
    "  if is_terminal(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else: #if this is a 'legal' starting location\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "    while not is_terminal(current_row_index, current_column_index):\n",
    "      #get the best action to take\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "      #move to the next location on the path, and add the new location to the list\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path\n",
    "  \n",
    "  #define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "  current_row_index = np.random.randint(environment_rows)\n",
    "  current_column_index = np.random.randint(environment_cols)\n",
    "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
    "  #(i.e., until the chosen state is a 'white square').\n",
    "  while is_terminal(current_row_index, current_column_index):\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_cols)\n",
    "  return current_row_index, current_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "  #get the starting location for this episode\n",
    "  row_index, column_index = get_starting_location()\n",
    "\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "  while not is_terminal(row_index, column_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "    \n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 4], [3, 4], [3, 3], [2, 3], [1, 3], [1, 2], [0, 2], [0, 1], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(get_shortest_path(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "State    |   N       E       S       W\n",
      "----------------------------------------\n",
      "(0,0)   |  0.00   0.00   0.00   0.00  \n",
      "(0,1)   |  8.00   6.20   -2.80  10.00 \n",
      "(0,2)   |  6.20   4.58   4.58   8.00  \n",
      "(0,3)   |  4.58   3.12   3.12   6.20  \n",
      "(0,4)   |  3.12   3.12   1.81   4.58  \n",
      "(1,0)   |  10.00  -2.80  -1.90  8.00  \n",
      "(1,1)   |  8.00   4.58   4.58   8.00  \n",
      "(1,2)   |  6.20   3.12   3.12   -2.80 \n",
      "(1,3)   |  4.58   1.81   1.81   4.58  \n",
      "(1,4)   |  3.12   1.81   0.63   3.12  \n",
      "(2,0)   |  8.00   4.58   4.58   6.20  \n",
      "(2,1)   |  -2.80  3.12   0.63   -2.80 \n",
      "(2,2)   |  4.58   1.81   1.81   1.81  \n",
      "(2,3)   |  3.12   0.63   0.63   3.12  \n",
      "(2,4)   |  1.81   0.61   -0.43  -2.52 \n",
      "(3,0)   |  -2.79  0.63   -1.36  -0.39 \n",
      "(3,1)   |  1.81   1.81   -0.43  -0.43 \n",
      "(3,2)   |  3.12   0.63   0.63   0.63  \n",
      "(3,3)   |  1.81   -0.43  -0.43  1.81  \n",
      "(3,4)   |  0.63   -0.43  -1.39  0.63  \n",
      "(4,0)   |  -0.43  -0.43  -1.38  -1.38 \n",
      "(4,1)   |  0.63   -1.79  -0.43  -1.29 \n",
      "(4,2)   |  1.81   -0.44  0.60   1.81  \n",
      "(4,3)   |  0.63   -1.39  -0.46  0.63  \n",
      "(4,4)   |  -0.43  -1.63  -1.39  -0.73 \n"
     ]
    }
   ],
   "source": [
    "# Printing Q-table\n",
    "print(\"Q-table:\")\n",
    "print(\"State    |   N       E       S       W\")\n",
    "print(\"----------------------------------------\")\n",
    "for row in range(environment_rows):\n",
    "    for col in range(environment_cols):\n",
    "        print(f\"({row},{col})   | \", end=\"\")\n",
    "        for action_idx, action in enumerate(actions):\n",
    "            q_value_str = f\"{q_values[row, col, action_idx]:.2f}\"  # Convert Q-value to string\n",
    "            padding = max(0, 7 - len(q_value_str))  # Calculate padding to center-align\n",
    "            print(\" \" * (padding // 2) + q_value_str + \" \" * ((padding + 1) // 2), end=\"\")\n",
    "        print()  # Move to the next line for the next state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epsilon = 0.9 # percentage of the time we take beset action instead of random action\n",
    "discount_factor = 0.5 #discount factor for future rewards\n",
    "learning_rate = 0.9 #rate at which the AI agent learns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
