{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 4368 Group Project :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Information for Jackson's implementation of Agent Class and environment space:\n",
    "* Gamestate is a list of integers, [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]. \n",
    "* Legal Moves is a list of Chars, [\"P\", \"D\", \"N\", \"E\", \"S\", \"W\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be the agent class that will create objects with a Q Table and an ID. \n",
    "\n",
    "It currently has three non-declaration functions \n",
    "1. getLegalMoves(): takes in gamestate and using the ID of the agent derives all legal moves for the agent. returns a list of capital letter chars\n",
    "2. updateGamestate(): takes in gamestate and nextmove and applies the next move to the game state. returns a list of integers representative of the environment\n",
    "3. step(): calls previous methods to produce the whole step of the agent given the game state. returns a list of integers representative of the environment and should update the agent's Qtable when appropriate\n",
    "\n",
    "Todo \n",
    "* implement the Q-Table\n",
    "* implement functions relating to the Q-Table, update, build, etc\n",
    "* Next Move according to policy needs to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table class uses a constructor that initializes the Q-table with zeros based on the size of the environment and learning parameters. The function get_q_value retrieves the Q-value for a given position-action pair, while update_q_value updates it using the Bellman equation considering the immediate reward and the next position. The print_q_table function displays the Q-table in a readable format, organizing Q-values for each state-action pair. The select_action function chooses an action according to specified policies: randomly from legal moves, with a probability-based exploitation of Q-values, or greedily exploiting Q-values. Finally, the exploit_action function selects an action by exploiting the Q-values, preferring the one with the highest Q-value for the position, randomly breaking ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters definitions within Q-Table functions:\n",
    "\n",
    "* position: A tuple representing the current position (row and column coordinates) of the agent in the environment.\n",
    "* action: An integer representing the action taken by the agent.\n",
    "* reward: The immediate reward received by the agent for taking an action.\n",
    "* next_position: A tuple representing the next position (row and column coordinates) of the agent after taking an action.\n",
    "* policy: A string representing the policy used for action selection.\n",
    "* legal_moves: A list of strings representing the legal moves available to the agent from the current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        # Initialize QTable with environment parameters and zero-initialized Q-values\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_cols = environment_cols\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((environment_rows, environment_cols, num_actions))\n",
    "\n",
    "    def get_q_value(self, position, action):\n",
    "        # Retrieve Q-value for a given position-action pair\n",
    "        return self.q_table[position[0], position[1], action]\n",
    "\n",
    "    def update_q_value(self, position, action, reward, next_position,learning_method):\n",
    "        # Update Q-value for a given position-action pair using the Bellman equation\n",
    "        current_q_value = self.q_table[position[0], position[1], action]\n",
    "        if learning_method != \"SARSA\"\n",
    "            max_next_q_value = np.max(self.q_table[next_position[0], next_position[1]])\n",
    "            new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "            self.q_table[position[0], position[1], action] = new_q_value\n",
    "        # Update Q-value for a given position-action pair using the sarsa update\n",
    "        else:\n",
    "            next_q_value = self.q_table[next_position[0], next_position[1], action] #Agent.nextstep\n",
    "            new_q_value =  current_q_value + self.learning_rate * (reward + (self.discount_factor*current_q_value) - next_q_value)\n",
    "            self.q_table[position[0], position[1], action] = new_q_value\n",
    "\n",
    "    def print_q_table(self):\n",
    "        # Print the Q-table in a readable format\n",
    "        print(\"Q-table:\")\n",
    "        print(\"State    |   N       E       S       W\")\n",
    "        print(\"----------------------------------------\")\n",
    "        for row in range(self.environment_rows):\n",
    "            for col in range(self.environment_cols):\n",
    "                print(f\"({row},{col})   | \", end=\"\")\n",
    "                for action_idx in range(self.num_actions):\n",
    "                    q_value_str = f\"{self.q_table[row, col, action_idx]:.2f}\"  \n",
    "                    padding = max(0, 7 - len(q_value_str))\n",
    "                    print(\" \" * (padding // 2) + q_value_str + \" \" * ((padding + 1) // 2), end=\"\")\n",
    "                print()\n",
    "\n",
    "    def select_action(self, position, policy, legal_moves):\n",
    "        # Select an action based on the specified policy\n",
    "        if policy == \"PRANDOM\":\n",
    "            return random.choice(legal_moves)\n",
    "        elif policy == \"PEXPLOIT\":\n",
    "            # With probability 0.8, exploit the Q-values; otherwise, choose a random action\n",
    "            if random.random() < 0.8:\n",
    "                return self.exploit_action(position, legal_moves)\n",
    "            else:\n",
    "                return random.choice(legal_moves)\n",
    "        elif policy == \"PGREEDY\":\n",
    "            # Greedily select the action with the highest Q-value\n",
    "            return self.exploit_action(position, legal_moves)\n",
    "\n",
    "    def exploit_action(self, position, legal_moves):\n",
    "        # Exploit the Q-values by selecting the action with the highest Q-value for the given position\n",
    "        max_q_value = np.max(self.q_table[position[0], position[1]])\n",
    "        max_indices = np.argwhere(self.q_table[position[0], position[1]] == max_q_value).flatten()\n",
    "        chosen_action_index = random.choice(max_indices)\n",
    "        return legal_moves[chosen_action_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    # this class is used to define the different agents\n",
    "\n",
    "    def __init__(self, agentID):\n",
    "        self.qtable = None              # To be filled out later\n",
    "        self.agentID = agentID          # Agent ID differentiates agents: Red=0, Blue=1, Black=2    \n",
    "        self.reward = 0                 # Represents the current rewards this agent has scored\n",
    "        self.rewardList = []            # Holds the rewards that the agent has scored for each step the agent has been through\n",
    "        self.nextstep = \"\"            #Holds the future action to be implemented in SARSA and Step function\n",
    "    # sets reward for picking up\n",
    "    def setPickupReward(self):\n",
    "        self.reward += 13\n",
    "\n",
    "    # sets reward for dropping off\n",
    "    def setDropoffReward(self):\n",
    "        self.reward += 13\n",
    "\n",
    "    # sets penalty for moving\n",
    "    def setMovePenalty(self):\n",
    "        self.reward -= 1\n",
    "\n",
    "    # appends current reward to the list of agent rewards for each step\n",
    "    def setRewardList(self, reward):\n",
    "        self.rewardList.append(reward)\n",
    "\n",
    "    def setAgentQTable(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        self.qtable = QTable(environment_rows, environment_cols, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "    # returns current rewards for agents\n",
    "    def getReward(self):\n",
    "        return self.reward\n",
    "\n",
    "    # returns list of \n",
    "    def getRewardList(self):\n",
    "        return self.rewardList\n",
    "\n",
    "    # returns the possible moves as a tuple of Chars, The tuple is important because the legalmoves should not be changed at any point in the step\n",
    "    # checks all possible options for the game state and is universal to all agents, always returns the legal i.e. possible moves for instance:\n",
    "    # dropping up when a box when agent has n o box is not possible -> x.agentid = 1 then D is not a legal move\n",
    "    # picking up when a box when agent has a box is not possible -> x.agentid = 1 then P is not a legal move\n",
    "    # moving into a wall -> i or j = 0 or 6 then corresponding move is not a legal move\n",
    "    # moving into another agent is not possible -> if (i+-1,j) or (i,j+-1) = other agent position then corresponding move is not a legal move\n",
    "    # if dropoff location has 5, D is not a legal move\n",
    "    # if pickup location has 0, P is not a legal move\n",
    "\n",
    "    def getLegalMoves(self, gamestate):\n",
    "        legalMoves = [\"P\", \"D\", \"N\", \"S\", \"E\", \"W\"]\n",
    "        positions = gamestate[0:6]\n",
    "\n",
    "        ipositions = [positions[0], positions[2], positions[4]]\n",
    "        agentI = ipositions.pop(self.agentID)\n",
    "\n",
    "        jpositions = [positions[1], positions[3],  positions[5]]\n",
    "        agentJ = jpositions.pop(self.agentID)\n",
    "\n",
    "        otherAgentPositions = [[ipositions[0], jpositions[0]], [ipositions[1], jpositions[1]]]\n",
    "\n",
    "        agentBox = gamestate[self.agentID+6]\n",
    "\n",
    "        dropoff = gamestate[9:12]\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "\n",
    "        pickup = gamestate[12:]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(agentBox == 0):\n",
    "            legalMoves.remove(\"D\")\n",
    "            if([agentI, agentJ] not in pickupPositions):\n",
    "                legalMoves.remove(\"P\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in pickupPositions:\n",
    "                    if ([agentI, agentJ] == location and pickup[index] == 0):\n",
    "                        legalMoves.remove(\"P\")\n",
    "                    index += 1\n",
    "        else:\n",
    "            legalMoves.remove(\"P\")\n",
    "            if([agentI, agentJ] not in dropoffPositions):\n",
    "                legalMoves.remove(\"D\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in dropoffPositions:\n",
    "                    if ([agentI, agentJ] == location and dropoff[index] == 5):\n",
    "                        legalMoves.remove(\"D\")\n",
    "                    index += 1\n",
    "\n",
    "        if([agentI-1, agentJ] in otherAgentPositions or agentI-1 == 0):\n",
    "            legalMoves.remove(\"N\")\n",
    "        if([agentI+1, agentJ] in otherAgentPositions or agentI+1 == 6):\n",
    "            legalMoves.remove(\"S\")\n",
    "        if([agentI, agentJ-1] in otherAgentPositions or agentJ-1 == 0):\n",
    "            legalMoves.remove(\"W\")   \n",
    "        if([agentI, agentJ+1] in otherAgentPositions or agentJ+1 == 6):\n",
    "            legalMoves.remove(\"E\")   \n",
    "\n",
    "        return legalMoves\n",
    "\n",
    "    # takes the nextmove and returns the corresponding gamestate as a list of integers\n",
    "    def getGameState(self, gamestate, nextMove):\n",
    "        agentI = gamestate[self.agentID*2]\n",
    "        agentJ = gamestate[self.agentID*2+1]\n",
    "\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(nextMove == \"D\"):\n",
    "            index = 0\n",
    "            for location in dropoffPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[9 + index] = gamestate[9 + index] + 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 0\n",
    "\n",
    "        if(nextMove == \"P\"):\n",
    "            index = 0\n",
    "            for location in pickupPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[12 + index] = gamestate[12 + index] - 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 1\n",
    "\n",
    "        if(nextMove == \"N\"):\n",
    "            gamestate[self.agentID*2] = agentI-1\n",
    "        if(nextMove == \"S\"):\n",
    "            gamestate[self.agentID*2] = agentI+1\n",
    "        if(nextMove == \"W\"):\n",
    "            gamestate[self.agentID*2+1] = agentI-1\n",
    "        if(nextMove == \"E\"):\n",
    "            gamestate[self.agentID*2+1] = agentI+1\n",
    "\n",
    "        return gamestate\n",
    "\n",
    "    def step(self, gamestate, policy,learning_method): #learning_method is a string_arg passed on as SARSA or empty\n",
    "        # Get legal moves for the agent\n",
    "        legal_moves = self.getLegalMoves(gamestate)\n",
    "        \n",
    "        # Select action based on the Q-table and policy, and set agent reward accordingly\n",
<<<<<<< Updated upstream
    "        if learning_method != \"SARSA\":\n",
    "            if \"P\" in legal_moves:\n",
    "                next_move = \"P\"\n",
    "                self.setPickupReward()\n",
    "            elif \"D\" in legal_moves:\n",
    "                next_move = \"D\"\n",
    "                self.setDropoffReward()\n",
    "            else:\n",
    "                # If neither pickup nor dropoff is available, use Q-table-based policy\n",
    "                next_move = self.qtable.select_action((gamestate[self.agentID*2], gamestate[self.agentID*2+1]), policy, legal_moves)\n",
    "                self.setMovePenalty()\n",
    "            \n",
    "        else:\n",
    "            new_legal_moves = self.getLegalMoves(new_game_state)\n",
    "            self.nextstep = self.qtable.select_action((new_game_state[self.agentID*2], new_game_state[self.agentID*2+1]), policy, new_legal_moves)\n",
    "            \n",
    "        \n",
    "        # Update game state based on the selected move \n",
    "        new_game_state = self.getGameState(gamestate, next_move)\n",
    "        \n",
=======
    "        if(len(legal_moves) == 0):\n",
    "            new_game_state = gamestate\n",
    "        elif \"P\" in legal_moves:\n",
    "            next_move = \"P\"\n",
    "            self.setPickupReward()\n",
    "            new_game_state = self.getGameState(gamestate, next_move)\n",
    "        elif \"D\" in legal_moves:\n",
    "            next_move = \"D\"\n",
    "            self.setDropoffReward()\n",
    "            new_game_state = self.getGameState(gamestate, next_move)\n",
    "        else:\n",
    "            # If neither pickup nor dropoff is available, use Q-table-based policy\n",
    "            next_move = self.qtable.select_action((gamestate[self.agentID*2], gamestate[self.agentID*2+1]), policy, legal_moves)\n",
    "            self.setMovePenalty()\n",
    "            new_game_state = self.getGameState(gamestate, next_move)\n",
    "\n",
    "        # Update game state based on the selected move \n",
    "        #new_game_state = self.getGameState(gamestate, next_move)\n",
>>>>>>> Stashed changes
    "\n",
    "        # Add step reward to agent reward list\n",
    "        self.setRewardList(self.getReward())\n",
    "        \n",
    "        \n",
    "        return new_game_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tIn Experiment 1 you use learning rate=0.3 and discount factor=0.5, and run the traditional Q-learning algorithm for 9000 steps; initially you run the policy PRANDOM for 500 steps, then\n",
    "* \tContinue running PRANDOM for 8500 more steps  (only policy will change, agents will keep the behavior from the first training)\n",
    "*\tRun PGREEDY for the remaining 8500 steps (only policy will change, agents will keep the behavior from the first training)\n",
    "*\tRun PEXPLOIT for the remaining 8500 steps (only policy will change, agents will keep the behavior from the first training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am going to run the first 500 steps of prandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Environment\n",
      "[4, 2, 4, 3, 2, 1, 0, 0, 0, 1, 2, 3, 4, 2, 3]\n",
      "After 500 Steps\n",
      "[4, 2, 4, 3, 2, 1, 0, 0, 0, 1, 2, 3, 4, 2, 3]\n",
      "Red Reward: -138\n",
      "Blue Reward: -110\n",
      "Black Reward: -82\n",
      "After 8500 Steps\n",
      "[3, 4, 5, 5, 4, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0]\n",
      "Red Reward: -2886\n",
      "Blue Reward: -2887\n",
      "Black Reward: -2802\n"
     ]
    }
   ],
   "source": [
    "environment_rows=5\n",
    "environment_cols=5\n",
    "num_actions = 4\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.5\n",
    "\n",
    "redAgent = Agent(0)\n",
    "redAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "blueAgent = Agent(1)\n",
    "blueAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "blackAgent = Agent(2)\n",
    "blackAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "\n",
    "initialEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "currEnvironment = initialEnvironment\n",
    "policy = \"PRANDOM\"\n",
    "\n",
    "first500 = int(500/3)\n",
    "for i in range(first500):\n",
    "    currEnvironment = redAgent.step(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.step(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.step(currEnvironment, policy)\n",
    "\n",
    "print(f\"Initial Environment\\n{initialEnvironment}\")\n",
    "print(f\"After 500 Steps\\n{currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n",
    "next8500 = int(8500/3)\n",
    "for i in range(next8500):\n",
    "    currEnvironment = redAgent.step(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.step(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.step(currEnvironment, policy)\n",
    "\n",
    "print(f\"After 8500 Steps\\n{currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
