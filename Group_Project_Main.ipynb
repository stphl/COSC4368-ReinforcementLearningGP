{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 4368 Group Project :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Information for Jackson's implementation of Agent Class and environment space:\n",
    "* Gamestate is a list of integers, [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]. \n",
    "* Legal Moves is a list of Chars, [\"P\", \"D\", \"N\", \"E\", \"S\", \"W\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be the agent class that will create objects with a Q Table and an ID. \n",
    "\n",
    "It currently has three non-declaration functions \n",
    "1. getLegalMoves(): takes in gamestate and using the ID of the agent derives all legal moves for the agent. returns a list of capital letter chars\n",
    "2. updateGamestate(): takes in gamestate and nextmove and applies the next move to the game state. returns a list of integers representative of the environment\n",
    "3. step(): calls previous methods to produce the whole step of the agent given the game state. returns a list of integers representative of the environment and should update the agent's Qtable when appropriate\n",
    "\n",
    "Todo \n",
    "* implement the Q-Table\n",
    "* implement functions relating to the Q-Table, update, build, etc\n",
    "* Next Move according to policy needs to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # this class is used to define the different agents\n",
    "\n",
    "    def __init__(self, agentID):\n",
    "        # self.qtable = None            To be filled out later\n",
    "        self.agentID = agentID\n",
    "    \n",
    "    def getLegalMoves(self, gamestate):\n",
    "        legalMoves = [\"P\", \"D\", \"N\", \"S\", \"E\", \"W\"]\n",
    "        positions = gamestate[0:6]\n",
    "\n",
    "        ipositions = [positions[0], positions[2], positions[4]]\n",
    "        agentI = ipositions.pop(self.agentID)\n",
    "\n",
    "        jpositions = [positions[1], positions[3],  positions[5]]\n",
    "        agentJ = jpositions.pop(self.agentID)\n",
    "\n",
    "        otherAgentPositions = [[ipositions[0], jpositions[0]], [ipositions[1], jpositions[1]]]\n",
    "\n",
    "        agentBox = gamestate[self.agentID+6]\n",
    "\n",
    "        dropoff = gamestate[9:12]\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "\n",
    "        pickup = gamestate[12:]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(agentBox == 0):\n",
    "            legalMoves.remove(\"D\")\n",
    "            if([agentI, agentJ] not in pickupPositions):\n",
    "                legalMoves.remove(\"P\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in pickupPositions:\n",
    "                    if ([agentI, agentJ] == location and pickup[index] == 0):\n",
    "                        legalMoves.remove(\"P\")\n",
    "                    index += 1\n",
    "        else:\n",
    "            legalMoves.remove(\"P\")\n",
    "            if([agentI, agentJ] not in dropoffPositions):\n",
    "                legalMoves.remove(\"D\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in dropoffPositions:\n",
    "                    if ([agentI, agentJ] == location and dropoff[index] == 5):\n",
    "                        legalMoves.remove(\"D\")\n",
    "                    index += 1\n",
    "\n",
    "        if([agentI-1, agentJ] in otherAgentPositions or agentI-1 == 0):\n",
    "            legalMoves.remove(\"N\")\n",
    "        if([agentI+1, agentJ] in otherAgentPositions or agentI+1 == 6):\n",
    "            legalMoves.remove(\"S\")\n",
    "        if([agentI, agentJ-1] in otherAgentPositions or agentJ-1 == 0):\n",
    "            legalMoves.remove(\"W\")   \n",
    "        if([agentI, agentJ+1] in otherAgentPositions or agentJ+1 == 6):\n",
    "            legalMoves.remove(\"E\")   \n",
    "\n",
    "        return legalMoves\n",
    "\n",
    "    def updateGameState(self, gamestate, nextMove):\n",
    "        agentI = gamestate[self.agentID*2]\n",
    "        agentJ = gamestate[self.agentID*2+1]\n",
    "\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(nextMove == \"D\"):\n",
    "            index = 0\n",
    "            for location in dropoffPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[9 + index] = gamestate[9 + index] + 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 0\n",
    "\n",
    "        if(nextMove == \"P\"):\n",
    "            index = 0\n",
    "            for location in pickupPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[12 + index] = gamestate[12 + index] - 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 1\n",
    "\n",
    "        if(nextMove == \"N\"):\n",
    "            gamestate[self.agentID*2] = agentI-1\n",
    "        if(nextMove == \"S\"):\n",
    "            gamestate[self.agentID*2] = agentI+1\n",
    "        if(nextMove == \"W\"):\n",
    "            gamestate[self.agentID*2+1] = agentI-1\n",
    "        if(nextMove == \"E\"):\n",
    "            gamestate[self.agentID*2+1] = agentI+1\n",
    "\n",
    "        return gamestate\n",
    "    \n",
    "    # def step(self, gamestate):\n",
    "    #     # Method representing one step of the agent in the game\n",
    "    #     legalmoves = self.getLegalMoves(gamestate)\n",
    "    #     nextMove = None                                   # will be changed when Q Table is implemented\n",
    "    #     # updateQTable(nextMove)\n",
    "    #     newGameState = self.updateGameState(gamestate, nextMove)\n",
    "    #     return newGameState\n",
    "\n",
    "    def step(self, gamestate, q_table, policy):\n",
    "        # Get legal moves for the agent\n",
    "        legal_moves = self.getLegalMoves(gamestate)\n",
    "        \n",
    "        # Select action based on the Q-table and policy\n",
    "        if \"P\" in legal_moves:\n",
    "            next_move = \"P\"\n",
    "        elif \"D\" in legal_moves:\n",
    "            next_move = \"D\"\n",
    "        else:\n",
    "            # If neither pickup nor dropoff is available, use Q-table-based policy\n",
    "            next_move = q_table.select_action((gamestate[self.agentID*2], gamestate[self.agentID*2+1]), policy, legal_moves)\n",
    "        \n",
    "        # Update game state based on the selected move\n",
    "        new_game_state = self.updateGameState(gamestate, next_move)\n",
    "        \n",
    "        return new_game_state\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "redAgent = Agent(0)\n",
    "blueAgent = Agent(1)\n",
    "blackAgent = Agent(2)\n",
    "\n",
    "initialEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table class uses a constructor that initializes the Q-table with zeros based on the size of the environment and learning parameters. The function get_q_value retrieves the Q-value for a given state-action pair, while update_q_value updates it using the Bellman equation considering the immediate reward and the next state. The print_q_table function displays the Q-table in a readable format, organizing Q-values for each state-action pair. The select_action function chooses an action according to specified policies: randomly from legal moves, with a probability-based exploitation of Q-values, or greedily exploiting Q-values. Finally, the exploit_action function selects an action by exploiting the Q-values, preferring the one with the highest Q-value for the state, randomly breaking ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters definitions within Q-Table functions:\n",
    "\n",
    "* state: A tuple representing the current state (coordinates) of the agent in the environment.\n",
    "* action: An integer representing the action taken by the agent.\n",
    "* reward: The immediate reward received by the agent for taking an action.\n",
    "* next_state: A tuple representing the next state (coordinates) of the agent after taking an action.\n",
    "* policy: A string representing the policy used for action selection.\n",
    "* legal_moves: A list of strings representing the legal moves available to the agent from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        # Initialize QTable with environment parameters and zero-initialized Q-values\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_cols = environment_cols\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((environment_rows, environment_cols, num_actions))\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        # Retrieve Q-value for a given state-action pair\n",
    "        return self.q_table[state[0], state[1], action]\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        # Update Q-value for a given state-action pair using the Bellman equation\n",
    "        current_q_value = self.q_table[state[0], state[1], action]\n",
    "        max_next_q_value = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "        self.q_table[state[0], state[1], action] = new_q_value\n",
    "\n",
    "    def print_q_table(self):\n",
    "        # Print the Q-table in a readable format\n",
    "        print(\"Q-table:\")\n",
    "        print(\"State    |   N       E       S       W\")\n",
    "        print(\"----------------------------------------\")\n",
    "        for row in range(self.environment_rows):\n",
    "            for col in range(self.environment_cols):\n",
    "                print(f\"({row},{col})   | \", end=\"\")\n",
    "                for action_idx in range(self.num_actions):\n",
    "                    q_value_str = f\"{self.q_table[row, col, action_idx]:.2f}\"  \n",
    "                    padding = max(0, 7 - len(q_value_str))\n",
    "                    print(\" \" * (padding // 2) + q_value_str + \" \" * ((padding + 1) // 2), end=\"\")\n",
    "                print()\n",
    "\n",
    "    def select_action(self, state, policy, legal_moves):\n",
    "        # Select an action based on the specified policy\n",
    "        if policy == \"PRANDOM\":\n",
    "            return random.choice(legal_moves)\n",
    "        elif policy == \"PEXPLOIT\":\n",
    "            # With probability 0.8, exploit the Q-values; otherwise, choose a random action\n",
    "            if random.random() < 0.8:\n",
    "                return self.exploit_action(state, legal_moves)\n",
    "            else:\n",
    "                return random.choice(legal_moves)\n",
    "        elif policy == \"PGREEDY\":\n",
    "            # Greedily select the action with the highest Q-value\n",
    "            return self.exploit_action(state, legal_moves)\n",
    "\n",
    "    def exploit_action(self, state, legal_moves):\n",
    "        # Exploit the Q-values by selecting the action with the highest Q-value for the given state\n",
    "        max_q_value = np.max(self.q_table[state[0], state[1]])\n",
    "        max_indices = np.argwhere(self.q_table[state[0], state[1]] == max_q_value).flatten()\n",
    "        chosen_action_index = random.choice(max_indices)\n",
    "        return legal_moves[chosen_action_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
