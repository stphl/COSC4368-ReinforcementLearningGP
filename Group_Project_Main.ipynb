{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 4368 Group Project :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Information for Jackson's implementation of Agent Class and environment space:\n",
    "* Gamestate is a list of integers, [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]. \n",
    "* Legal Moves is a list of Chars, [\"P\", \"D\", \"N\", \"E\", \"S\", \"W\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be the agent class that will create objects with a Q Table and an ID. \n",
    "\n",
    "It currently has three non-declaration functions \n",
    "1. getLegalMoves(): takes in gamestate and using the ID of the agent derives all legal moves for the agent. returns a list of capital letter chars\n",
    "2. updateGamestate(): takes in gamestate and nextmove and applies the next move to the game state. returns a list of integers representative of the environment\n",
    "3. step(): calls previous methods to produce the whole step of the agent given the game state. returns a list of integers representative of the environment and should update the agent's Qtable when appropriate\n",
    "\n",
    "Todo \n",
    "* implement the Q-Table\n",
    "* implement functions relating to the Q-Table, update, build, etc\n",
    "* Next Move according to policy needs to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table class uses a constructor that initializes the Q-table with zeros based on the size of the environment and learning parameters. The function get_q_value retrieves the Q-value for a given position-action pair, while update_q_value updates it using the Bellman equation considering the immediate reward and the next position. The print_q_table function displays the Q-table in a readable format, organizing Q-values for each state-action pair. The select_action function chooses an action according to specified policies: randomly from legal moves, with a probability-based exploitation of Q-values, or greedily exploiting Q-values. Finally, the exploit_action function selects an action by exploiting the Q-values, preferring the one with the highest Q-value for the position, randomly breaking ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters definitions within Q-Table functions:\n",
    "\n",
    "* position: A tuple representing the current position (row and column coordinates) of the agent in the environment.\n",
    "* action: An integer representing the action taken by the agent.\n",
    "* reward: The immediate reward received by the agent for taking an action.\n",
    "* next_position: A tuple representing the next position (row and column coordinates) of the agent after taking an action.\n",
    "* policy: A string representing the policy used for action selection.\n",
    "* legal_moves: A list of strings representing the legal moves available to the agent from the current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        # Initialize QTable with environment parameters and zero-initialized Q-values\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_cols = environment_cols\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((environment_rows, environment_cols, num_actions))\n",
    "\n",
    "    def set_q_value(self, position, action, new_q_value):\n",
    "        action_index = self.get_action_index(action)\n",
    "        self.q_table[position[0]-1, (position[1]-1), action_index] = new_q_value\n",
    "\n",
    "    def get_q_value(self, position, action):\n",
    "        action_index = self.get_action_index(action)\n",
    "        iposition = position[0]-1\n",
    "        jposition = position[1]-1\n",
    "        # Retrieve Q-value for a given position-action pair\n",
    "        return self.q_table[iposition, jposition, action_index]\n",
    "\n",
    "    def calc_q_value(self, position, action, reward, next_position, next_legal_moves, next_step, learning_method):\n",
    "        if(action == \"\"):\n",
    "            return  # if no action do nothing I think\n",
    "        # Update Q-value for a given position-action pair using the Bellman equation\n",
    "        current_q_value = self.get_q_value(position, action)\n",
    "        if (learning_method == \"Q\"):\n",
    "            max_next_q_value = -10000\n",
    "            for move in next_legal_moves:\n",
    "                curr_next_q_value = self.get_q_value(next_position, move)\n",
    "                if (curr_next_q_value > max_next_q_value):\n",
    "                    max_next_q_value = curr_next_q_value\n",
    "            new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "            self.set_q_value(position, action, new_q_value) \n",
    "        # Update Q-value for a given position-action pair using the sarsa update\n",
    "        if(learning_method == \"S\"):\n",
    "            next_q_value = self.get_q_value(next_position, next_step) #Agent.nextstep\n",
    "            new_q_value =  current_q_value + self.learning_rate * (reward + (self.discount_factor*current_q_value) - next_q_value)\n",
    "            self.set_q_value(position, action, new_q_value)\n",
    "\n",
    "    def print_q_table(self):\n",
    "        # Print the Q-table in a readable format\n",
    "        print(\"Q-table:\")\n",
    "        print(\"State    |   N       E       S       W\")\n",
    "        print(\"----------------------------------------\")\n",
    "        for row in range(self.environment_rows):\n",
    "            for col in range(self.environment_cols):\n",
    "                print(f\"({row},{col})   | \", end=\"\")\n",
    "                for action_idx in range(self.num_actions):\n",
    "                    q_value_str = f\"{self.q_table[row, col, action_idx]:.2f}\"  \n",
    "                    padding = max(0, 7 - len(q_value_str))\n",
    "                    print(\" \" * (padding // 2) + q_value_str + \" \" * ((padding + 1) // 2), end=\"\")\n",
    "                print()\n",
    "\n",
    "    def select_action(self, position, policy, legal_moves):\n",
    "        if(len(legal_moves) == 0):\n",
    "            return \"\"\n",
    "        # Select an action based on the specified policy\n",
    "        if policy == \"PRANDOM\":\n",
    "            return random.choice(legal_moves)\n",
    "        elif policy == \"PEXPLOIT\":\n",
    "            # With probability 0.8, exploit the Q-values; otherwise, choose a random action\n",
    "            if random.random() < 0.8:\n",
    "                return self.get_best_move(position, legal_moves)\n",
    "            else:\n",
    "                return random.choice(legal_moves)\n",
    "        elif policy == \"PGREEDY\":\n",
    "            # Greedily select the action with the highest Q-value\n",
    "            return self.get_best_move(position, legal_moves)\n",
    "\n",
    "    def get_best_move(self, position, legal_moves):\n",
    "        if(\"P\" in legal_moves):\n",
    "            return \"P\"\n",
    "        if(\"D\" in legal_moves):\n",
    "           return \"D\"\n",
    "        \n",
    "        # Exploit the Q-values by selecting the action with the highest Q-value for the given position\n",
    "        max_q_value = -10000\n",
    "        best_move = \"\"\n",
    "        for move in legal_moves:\n",
    "            curr_q_value = self.get_q_value(position, move)\n",
    "            if(curr_q_value > max_q_value):\n",
    "                max_q_value = curr_q_value\n",
    "                best_move = move\n",
    "        return best_move\n",
    "    \n",
    "    def get_action_index(self, action):\n",
    "        if(action == \"P\"):\n",
    "            index = 0\n",
    "        elif(action == \"D\"):\n",
    "            index = 1\n",
    "        elif(action == \"N\"):\n",
    "            index = 2\n",
    "        elif(action == \"E\"):\n",
    "            index = 3\n",
    "        elif(action == \"S\"):\n",
    "            index = 4\n",
    "        elif(action == \"W\"):\n",
    "            index = 5\n",
    "        else:\n",
    "            print(f\"Action: {action}\")\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be deleted later \n",
    "def update_q_value(self, position, action, reward, next_position, next_legal_moves, learning_method):\n",
    "    current_q_value = self.q_table[position[0], position[1], action]\n",
    "    if (learning_method == \"Q\"):\n",
    "        next_q_value = self.q_table[next_position[0], next_position[1], action] #Agent.nextstep\n",
    "        new_q_value =  current_q_value + self.learning_rate * (reward + (self.discount_factor*current_q_value) - next_q_value)\n",
    "        self.q_table[position[0], position[1], action] = new_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    # this class is used to define the different agents\n",
    "\n",
    "    def __init__(self, agentID):\n",
    "        self.qtable = None              # To be filled out later\n",
    "        self.agentID = agentID          # Agent ID differentiates agents: Red=0, Blue=1, Black=2    \n",
    "        self.reward = 0                 # Represents the current rewards this agent has scored\n",
    "        self.rewardList = []            # Holds the rewards that the agent has scored for each step the agent has been through\n",
    "        self.nextstep = \"\"            #Holds the future action to be implemented in SARSA and Step function\n",
    "    \n",
    "    # sets reward for picking up\n",
    "    def setPickupReward(self):\n",
    "        self.reward += 13\n",
    "\n",
    "    # sets reward for dropping off\n",
    "    def setDropoffReward(self):\n",
    "        self.reward += 13\n",
    "\n",
    "    # sets penalty for moving\n",
    "    def setMovePenalty(self):\n",
    "        self.reward -= 1\n",
    "\n",
    "    # appends current reward to the list of agent rewards for each step\n",
    "    def setRewardList(self, reward):\n",
    "        self.rewardList.append(reward)\n",
    "\n",
    "    def setAgentQTable(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        self.qtable = QTable(environment_rows, environment_cols, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "    def setAgentNextStep(self, nextStep):\n",
    "        self.nextstep = nextStep\n",
    "\n",
    "    def getAgentNextStep(self):\n",
    "        return self.nextstep\n",
    "    \n",
    "    # returns current rewards for agents\n",
    "    def getReward(self):\n",
    "        return self.reward\n",
    "\n",
    "    # returns list of \n",
    "    def getRewardList(self):\n",
    "        return self.rewardList\n",
    "    \n",
    "    def getPosition(self, gamestate):\n",
    "        agentI = gamestate[self.agentID*2]\n",
    "        agentJ = gamestate[self.agentID*2+1]\n",
    "\n",
    "        return [agentI,agentJ]\n",
    "\n",
    "    def getNextPosition(self, position, nextMove):\n",
    "        if(nextMove == \"N\"):\n",
    "            position[0] = position[0]-1\n",
    "        if(nextMove == \"S\"):\n",
    "            position[0] = position[0]+1\n",
    "        if(nextMove == \"W\"):\n",
    "            position[1] = position[1]-1\n",
    "        if(nextMove == \"E\"):\n",
    "            position[1] = position[1]+1\n",
    "\n",
    "        return position\n",
    "\n",
    "    # returns the possible moves as a tuple of Chars, The tuple is important because the legalmoves should not be changed at any point in the step\n",
    "    # checks all possible options for the game state and is universal to all agents, always returns the legal i.e. possible moves for instance:\n",
    "    # dropping up when a box when agent has n o box is not possible -> x.agentid = 1 then D is not a legal move\n",
    "    # picking up when a box when agent has a box is not possible -> x.agentid = 1 then P is not a legal move\n",
    "    # moving into a wall -> i or j = 0 or 6 then corresponding move is not a legal move\n",
    "    # moving into another agent is not possible -> if (i+-1,j) or (i,j+-1) = other agent position then corresponding move is not a legal move\n",
    "    # if dropoff location has 5, D is not a legal move\n",
    "    # if pickup location has 0, P is not a legal move\n",
    "\n",
    "    def getLegalMoves(self, gamestate):\n",
    "        legalMoves = [\"P\", \"D\", \"N\", \"S\", \"E\", \"W\"]\n",
    "        positions = gamestate[0:6]\n",
    "        position = self.getPosition(gamestate)\n",
    "\n",
    "        ipositions = [positions[0], positions[2], positions[4]]\n",
    "        agentI = ipositions.pop(self.agentID)\n",
    "\n",
    "        jpositions = [positions[1], positions[3],  positions[5]]\n",
    "        agentJ = jpositions.pop(self.agentID)\n",
    "\n",
    "        otherAgentPositions = [[ipositions[0], jpositions[0]], [ipositions[1], jpositions[1]]]\n",
    "\n",
    "        agentBox = gamestate[self.agentID+6]\n",
    "\n",
    "        dropoff = gamestate[9:12]\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "\n",
    "        pickup = gamestate[12:]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(agentBox == 0):\n",
    "            legalMoves.remove(\"D\")\n",
    "            if([agentI, agentJ] not in pickupPositions):\n",
    "                legalMoves.remove(\"P\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in pickupPositions:\n",
    "                    if ([agentI, agentJ] == location and pickup[index] == 0):\n",
    "                        legalMoves.remove(\"P\")\n",
    "                    index += 1\n",
    "        else:\n",
    "            legalMoves.remove(\"P\")\n",
    "            if([agentI, agentJ] not in dropoffPositions):\n",
    "                legalMoves.remove(\"D\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in dropoffPositions:\n",
    "                    if ([agentI, agentJ] == location and dropoff[index] == 5):\n",
    "                        legalMoves.remove(\"D\")\n",
    "                    index += 1\n",
    "\n",
    "        if([agentI-1, agentJ] in otherAgentPositions or agentI-1 == 0):\n",
    "            legalMoves.remove(\"N\")\n",
    "        if([agentI+1, agentJ] in otherAgentPositions or agentI+1 == 6):\n",
    "            legalMoves.remove(\"S\")\n",
    "        if([agentI, agentJ-1] in otherAgentPositions or agentJ-1 == 0):\n",
    "            legalMoves.remove(\"W\")   \n",
    "        if([agentI, agentJ+1] in otherAgentPositions or agentJ+1 == 6):\n",
    "            legalMoves.remove(\"E\")   \n",
    "\n",
    "        return legalMoves\n",
    "\n",
    "    # takes the nextmove and returns the corresponding gamestate as a list of integers\n",
    "    def getGameState(self, gamestate, nextMove):\n",
    "        agentI = gamestate[self.agentID*2]\n",
    "        agentJ = gamestate[self.agentID*2+1]\n",
    "\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(nextMove == \"D\"):\n",
    "            index = 0\n",
    "            for location in dropoffPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[9+index] = gamestate[9+index] + 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 0\n",
    "\n",
    "        if(nextMove == \"P\"):\n",
    "            index = 0\n",
    "            for location in pickupPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[12+index] = gamestate[12+index] - 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 1\n",
    "\n",
    "        if(nextMove == \"N\"):\n",
    "            gamestate[self.agentID*2] = agentI-1\n",
    "        if(nextMove == \"S\"):\n",
    "            gamestate[self.agentID*2] = agentI+1\n",
    "        if(nextMove == \"W\"):\n",
    "            gamestate[self.agentID*2+1] = agentJ-1\n",
    "        if(nextMove == \"E\"):\n",
    "            gamestate[self.agentID*2+1] = agentJ+1\n",
    "\n",
    "        return gamestate\n",
    "\n",
    "    def stepS(self, gamestate, policy):\n",
    "        learning_method = \"S\"\n",
    "\n",
    "        next_move = self.getAgentNextStep()\n",
    "        position = self.getPosition(gamestate)\n",
    "        \n",
    "        if(next_move == \"\"):\n",
    "            legal_moves = self.getLegalMoves(gamestate)\n",
    "            if(\"P\" in legal_moves):\n",
    "                next_move = \"P\"\n",
    "            elif(\"D\" in legal_moves):\n",
    "                next_move = \"D\"\n",
    "            else:\n",
    "                next_move = random.choice(legal_moves)\n",
    "\n",
    "        next_position = position\n",
    "        next_gamestate = self.getGameState(gamestate, next_move)\n",
    "        next_legal_moves = self.getLegalMoves(next_gamestate)\n",
    "\n",
    "        if(next_move == \"P\"):\n",
    "            self.setPickupReward()\n",
    "            turn_reward = 13\n",
    "        elif(next_move == \"D\"):\n",
    "            self.setDropoffReward()\n",
    "            turn_reward = 13\n",
    "        else:\n",
    "            self.setMovePenalty()\n",
    "            next_position = self.getNextPosition(position, next_move)\n",
    "            next_gamestate = self.getGameState(gamestate, next_move)\n",
    "            next_legal_moves = self.getLegalMoves(next_gamestate)\n",
    "            turn_reward = -1\n",
    "\n",
    "        agent_next_step = self.qtable.get_best_move(next_position, next_legal_moves)    \n",
    "        self.setAgentNextStep(agent_next_step)\n",
    "\n",
    "        self.qtable.calc_q_value(position, next_move, turn_reward, next_position, next_legal_moves, agent_next_step, learning_method)\n",
    "        self.setRewardList(self.getReward())\n",
    "        \n",
    "        return next_gamestate\n",
    "\n",
    "\n",
    "    def stepQ(self, gamestate, policy): #learning_method is a string_arg passed on as SARSA or empty\n",
    "        learning_method = \"Q\"\n",
    "        # Get legal moves for the agent\n",
    "        legal_moves = self.getLegalMoves(gamestate)\n",
    "        position = self.getPosition(gamestate)\n",
    "        \n",
    "        # Select action based on the Q-table and policy, and set agent reward accordingly\n",
    "        if \"P\" in legal_moves:\n",
    "            next_move = \"P\"\n",
    "            self.setPickupReward()\n",
    "            turn_reward=13\n",
    "        elif \"D\" in legal_moves:\n",
    "            next_move = \"D\"\n",
    "            self.setDropoffReward()\n",
    "            turn_reward=13\n",
    "        else: \n",
    "            # If neither pickup nor dropoff is available, use Q-table-based policy\n",
    "            next_move = self.qtable.select_action(position, policy, legal_moves)\n",
    "            self.setMovePenalty()\n",
    "            turn_reward=-1\n",
    "        \n",
    "        next_position = self.getNextPosition(position, next_move)\n",
    "        next_gamestate = self.getGameState(gamestate, next_move)\n",
    "        next_legal_moves = self.getLegalMoves(next_gamestate)\n",
    "\n",
    "        self.qtable.calc_q_value(position, next_move, turn_reward, next_position, next_legal_moves, \"NA\", learning_method)\n",
    "        self.setRewardList(self.getReward())\n",
    "        \n",
    "        return next_gamestate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishCheck(gamestate):\n",
    "    if(gamestate[9:] == [5,5,5,0,0,0]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tIn Experiment 1 you use learning rate=0.3 and discount factor=0.5, and run the traditional Q-learning algorithm for 9000 steps; initially you run the policy PRANDOM for 500 steps, then\n",
    "* \tContinue running PRANDOM for 8500 more steps  (only policy will change, agents will keep the behavior from the first training)\n",
    "*\tRun PGREEDY for the remaining 8500 steps (only policy will change, agents will keep the behavior from the first training)\n",
    "*\tRun PEXPLOIT for the remaining 8500 steps (only policy will change, agents will keep the behavior from the first training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am going to run the first 500 steps of prandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Random Run: [2, 2, 1, 3, 4, 1, 0, 0, 1, 2, 4, 3, 5, 0, 0]\n",
      "Red Reward: -110\n",
      "Blue Reward: -54\n",
      "Black Reward: -68\n",
      "Second Run: [3, 4, 4, 4, 3, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0]\n",
      "Red Reward: -2915\n",
      "Blue Reward: -2831\n",
      "Black Reward: -2831\n",
      "Greedy Run: [2, 2, 4, 3, 2, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0]\n",
      "Red Reward: -5748\n",
      "Blue Reward: -5664\n",
      "Black Reward: -5664\n",
      "Exploit Run: [3, 2, 4, 4, 3, 5, 0, 0, 0, 5, 5, 5, 0, 0, 0]\n",
      "Red Reward: -8581\n",
      "Blue Reward: -8497\n",
      "Black Reward: -8497\n"
     ]
    }
   ],
   "source": [
    "environment_rows=5\n",
    "environment_cols=5\n",
    "num_actions = 6\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.5\n",
    "\n",
    "redAgent = Agent(0)\n",
    "redAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "blueAgent = Agent(1)\n",
    "blueAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "blackAgent = Agent(2)\n",
    "blackAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "\n",
    "initialEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "testEnvironment =    [1,2,3,4,5,6,7,8,9,0,1,2,3,4,5]\n",
    "currEnvironment = initialEnvironment\n",
    "policy = \"PRANDOM\"\n",
    "\n",
    "first500 = int(500/3)\n",
    "for i in range(first500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "\n",
    "print(f\"First Random Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n",
    "\n",
    "# Second Round\n",
    "currEnvironment = initialEnvironment\n",
    "random8500 = int(8500/3)\n",
    "for i in range(random8500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "    #if(finishCheck(currEnvironment)):\n",
    "     #   print(\"finished\")\n",
    "      #  break\n",
    "\n",
    "print(f\"Second Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n",
    "\n",
    "# Third Round\n",
    "currEnvironment = initialEnvironment\n",
    "greedy8500 = int(8500/3)\n",
    "policy = \"PGREEDY\"\n",
    "for i in range(greedy8500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "\n",
    "print(f\"Greedy Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n",
    "\n",
    "currEnvironment = initialEnvironment\n",
    "exploit8500 = int(8500/3)\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "\n",
    "print(f\"Exploit Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
