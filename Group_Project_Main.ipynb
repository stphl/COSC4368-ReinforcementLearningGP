{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 4368 Group Project :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Information for Jackson's implementation of Agent Class and environment space:\n",
    "* Gamestate is a list of integers, [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]. \n",
    "* Legal Moves is a list of Chars, [\"P\", \"D\", \"N\", \"E\", \"S\", \"W\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be the agent class that will create objects with a Q Table and an ID. \n",
    "\n",
    "It currently has three non-declaration functions \n",
    "1. getLegalMoves(): takes in gamestate and using the ID of the agent derives all legal moves for the agent. returns a list of capital letter chars\n",
    "2. updateGamestate(): takes in gamestate and nextmove and applies the next move to the game state. returns a list of integers representative of the environment\n",
    "3. step(): calls previous methods to produce the whole step of the agent given the game state. returns a list of integers representative of the environment and should update the agent's Qtable when appropriate\n",
    "\n",
    "Todo \n",
    "* implement the Q-Table\n",
    "* implement functions relating to the Q-Table, update, build, etc\n",
    "* Next Move according to policy needs to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table class uses a constructor that initializes the Q-table with zeros based on the size of the environment and learning parameters. The function get_q_value retrieves the Q-value for a given position-action pair, while update_q_value updates it using the Bellman equation considering the immediate reward and the next position. The print_q_table function displays the Q-table in a readable format, organizing Q-values for each state-action pair. The select_action function chooses an action according to specified policies: randomly from legal moves, with a probability-based exploitation of Q-values, or greedily exploiting Q-values. Finally, the exploit_action function selects an action by exploiting the Q-values, preferring the one with the highest Q-value for the position, randomly breaking ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters definitions within Q-Table functions:\n",
    "\n",
    "* position: A tuple representing the current position (row and column coordinates) of the agent in the environment.\n",
    "* action: An integer representing the action taken by the agent.\n",
    "* reward: The immediate reward received by the agent for taking an action.\n",
    "* next_position: A tuple representing the next position (row and column coordinates) of the agent after taking an action.\n",
    "* policy: A string representing the policy used for action selection.\n",
    "* legal_moves: A list of strings representing the legal moves available to the agent from the current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        # Initialize QTable with environment parameters and zero-initialized Q-values\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_cols = environment_cols\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table = np.zeros((environment_rows, environment_cols, num_actions))\n",
    "\n",
    "    def set_q_value(self, position, action, new_q_value):\n",
    "        action_index = self.get_action_index(action)\n",
    "        self.q_table[position[0]-2, (position[1]-1), action_index] = new_q_value\n",
    "\n",
    "    def get_q_value(self, position, action):\n",
    "        action_index = self.get_action_index(action)\n",
    "        iposition = position[0]-1\n",
    "        jposition = position[1]-1\n",
    "        # Retrieve Q-value for a given position-action pair\n",
    "        return self.q_table[iposition, jposition, action_index]\n",
    "\n",
    "    def calc_q_value(self, position, action, reward, next_position, next_legal_moves, next_step, learning_method):\n",
    "        if(action == \"\"):\n",
    "            return  # if no action do nothing I think\n",
    "        # Update Q-value for a given position-action pair using the Bellman equation\n",
    "        current_q_value = self.get_q_value(position, action)\n",
    "        if (learning_method == \"Q\"):\n",
    "            max_next_q_value = -10000\n",
    "            for move in next_legal_moves:\n",
    "                curr_next_q_value = self.get_q_value(next_position, move)\n",
    "                if (curr_next_q_value > max_next_q_value):\n",
    "                    max_next_q_value = curr_next_q_value\n",
    "            new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value)\n",
    "            self.set_q_value(position, action, new_q_value) \n",
    "        # Update Q-value for a given position-action pair using the sarsa update\n",
    "        if(learning_method == \"S\"):\n",
    "            next_q_value = self.get_q_value(next_position, next_step) #Agent.nextstep\n",
    "            new_q_value =  current_q_value + self.learning_rate * (reward + (self.discount_factor*current_q_value) - next_q_value)\n",
    "            self.set_q_value(position, action, new_q_value)\n",
    "\n",
    "    def print_q_table(self):\n",
    "        # Print the Q-table in a readable format\n",
    "        print(\"Q-table:\")\n",
    "        print(\"State   |     P       D      N      E      S      W\")\n",
    "        print(\"------------------------------------------------------\")\n",
    "        for row in range(self.environment_rows):\n",
    "            for col in range(self.environment_cols):\n",
    "                print(f\"({row},{col})   | \", end=\"\")\n",
    "                for action_idx in range(self.num_actions):\n",
    "                    q_value_str = f\"{self.q_table[row, col, action_idx]:>7.2f}\"  \n",
    "                    print(q_value_str, end=\"\")\n",
    "                print()\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, position, policy, legal_moves):\n",
    "        if(len(legal_moves) == 0):\n",
    "            return \"\"\n",
    "        # Select an action based on the specified policy\n",
    "        if policy == \"PRANDOM\":\n",
    "            return random.choice(legal_moves)\n",
    "        elif policy == \"PEXPLOIT\":\n",
    "            # With probability 0.8, exploit the Q-values; otherwise, choose a random action\n",
    "            if random.random() < 0.8:\n",
    "                return self.get_best_move(position, legal_moves)\n",
    "            else:\n",
    "                return random.choice(legal_moves)\n",
    "        elif policy == \"PGREEDY\":\n",
    "            # Greedily select the action with the highest Q-value\n",
    "            return self.get_best_move(position, legal_moves)\n",
    "\n",
    "    def get_best_move(self, position, legal_moves):\n",
    "        if(\"P\" in legal_moves):\n",
    "            return \"P\"\n",
    "        if(\"D\" in legal_moves):\n",
    "           return \"D\"\n",
    "        \n",
    "        # Exploit the Q-values by selecting the action with the highest Q-value for the given position\n",
    "        max_q_value = -10000\n",
    "        best_move = \"\"\n",
    "        for move in legal_moves:\n",
    "            curr_q_value = self.get_q_value(position, move)\n",
    "            if(curr_q_value > max_q_value):\n",
    "                max_q_value = curr_q_value\n",
    "                best_move = move\n",
    "        return best_move\n",
    "    \n",
    "    def get_action_index(self, action):\n",
    "        if(action == \"P\"):\n",
    "            index = 0\n",
    "        elif(action == \"D\"):\n",
    "            index = 1\n",
    "        elif(action == \"N\"):\n",
    "            index = 2\n",
    "        elif(action == \"E\"):\n",
    "            index = 3\n",
    "        elif(action == \"S\"):\n",
    "            index = 4\n",
    "        elif(action == \"W\"):\n",
    "            index = 5\n",
    "        else:\n",
    "            print(f\"Action: {action}\")\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    # this class is used to define the different agents\n",
    "\n",
    "    def __init__(self, agentID):\n",
    "        self.qtable = None              # To be filled out later\n",
    "        self.agentID = agentID          # Agent ID differentiates agents: Red=0, Blue=1, Black=2    \n",
    "        self.reward = 0                 # Represents the current rewards this agent has scored\n",
    "        self.rewardList = []            # Holds the rewards that the agent has scored for each step the agent has been through\n",
    "        self.nextstep = \"\"            #Holds the future action to be implemented in SARSA and Step function\n",
    "    \n",
    "    # sets reward for picking up\n",
    "    def setPickupReward(self):\n",
    "        self.reward += 13\n",
    "\n",
    "    # sets reward for dropping off\n",
    "    def setDropoffReward(self):\n",
    "        self.reward += 13\n",
    "\n",
    "    # sets penalty for moving\n",
    "    def setMovePenalty(self):\n",
    "        self.reward -= 1\n",
    "\n",
    "    # appends current reward to the list of agent rewards for each step\n",
    "    def setRewardList(self, reward):\n",
    "        self.rewardList.append(reward)\n",
    "\n",
    "    def setAgentQTable(self, environment_rows, environment_cols, num_actions, learning_rate, discount_factor):\n",
    "        self.qtable = QTable(environment_rows, environment_cols, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "    def setAgentNextStep(self, nextStep):\n",
    "        self.nextstep = nextStep\n",
    "\n",
    "    def getAgentNextStep(self):\n",
    "        return self.nextstep\n",
    "    \n",
    "    # returns current rewards for agents\n",
    "    def getReward(self):\n",
    "        return self.reward\n",
    "\n",
    "    # returns list of \n",
    "    def getRewardList(self):\n",
    "        return self.rewardList\n",
    "    \n",
    "    def getPosition(self, gamestate):\n",
    "        agentI = gamestate[self.agentID*2]\n",
    "        agentJ = gamestate[self.agentID*2+1]\n",
    "\n",
    "        return [agentI,agentJ]\n",
    "\n",
    "    def getNextPosition(self, position, nextMove):\n",
    "        if(nextMove == \"N\"):\n",
    "            position[0] = position[0]-1\n",
    "        if(nextMove == \"S\"):\n",
    "            position[0] = position[0]+1\n",
    "        if(nextMove == \"W\"):\n",
    "            position[1] = position[1]-1\n",
    "        if(nextMove == \"E\"):\n",
    "            position[1] = position[1]+1\n",
    "\n",
    "        return position\n",
    "    \n",
    "    '''\n",
    "    # returns the possible moves as a tuple of Chars, The tuple is important because the legalmoves should not be changed at any point in the step\n",
    "    # checks all possible options for the game state and is universal to all agents, always returns the legal i.e. possible moves for instance:\n",
    "    # dropping up when a box when agent has n o box is not possible -> x.agentid = 1 then D is not a legal move\n",
    "    # picking up when a box when agent has a box is not possible -> x.agentid = 1 then P is not a legal move\n",
    "    # moving into a wall -> i or j = 0 or 6 then corresponding move is not a legal move\n",
    "    # moving into another agent is not possible -> if (i+-1,j) or (i,j+-1) = other agent position then corresponding move is not a legal move\n",
    "    # if dropoff location has 5, D is not a legal move\n",
    "    # if pickup location has 0, P is not a legal move   '''\n",
    "    def getLegalMoves(self, gamestate):\n",
    "        legalMoves = [\"P\", \"D\", \"N\", \"S\", \"E\", \"W\"]\n",
    "        positions = gamestate[0:6]\n",
    "        position = self.getPosition(gamestate)\n",
    "\n",
    "        ipositions = [positions[0], positions[2], positions[4]]\n",
    "        agentI = ipositions.pop(self.agentID)\n",
    "\n",
    "        jpositions = [positions[1], positions[3],  positions[5]]\n",
    "        agentJ = jpositions.pop(self.agentID)\n",
    "\n",
    "        otherAgentPositions = [[ipositions[0], jpositions[0]], [ipositions[1], jpositions[1]]]\n",
    "\n",
    "        agentBox = gamestate[self.agentID+6]\n",
    "\n",
    "        dropoff = gamestate[9:12]\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "\n",
    "        pickup = gamestate[12:]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(agentBox == 0):\n",
    "            legalMoves.remove(\"D\")\n",
    "            if([agentI, agentJ] not in pickupPositions):\n",
    "                legalMoves.remove(\"P\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in pickupPositions:\n",
    "                    if ([agentI, agentJ] == location and pickup[index] == 0):\n",
    "                        legalMoves.remove(\"P\")\n",
    "                    index += 1\n",
    "        else:\n",
    "            legalMoves.remove(\"P\")\n",
    "            if([agentI, agentJ] not in dropoffPositions):\n",
    "                legalMoves.remove(\"D\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in dropoffPositions:\n",
    "                    if ([agentI, agentJ] == location and dropoff[index] == 5):\n",
    "                        legalMoves.remove(\"D\")\n",
    "                    index += 1\n",
    "\n",
    "        if([agentI-1, agentJ] in otherAgentPositions or agentI-1 == 0):\n",
    "            legalMoves.remove(\"N\")\n",
    "        if([agentI+1, agentJ] in otherAgentPositions or agentI+1 == 6):\n",
    "            legalMoves.remove(\"S\")\n",
    "        if([agentI, agentJ-1] in otherAgentPositions or agentJ-1 == 0):\n",
    "            legalMoves.remove(\"W\")   \n",
    "        if([agentI, agentJ+1] in otherAgentPositions or agentJ+1 == 6):\n",
    "            legalMoves.remove(\"E\")   \n",
    "\n",
    "        return legalMoves\n",
    "\n",
    "    # this uses pickup locations for experiment 4\n",
    "    def getLegalMoves4(self, gamestate):\n",
    "        legalMoves = [\"P\", \"D\", \"N\", \"S\", \"E\", \"W\"]\n",
    "        positions = gamestate[0:6]\n",
    "        position = self.getPosition(gamestate)\n",
    "\n",
    "        ipositions = [positions[0], positions[2], positions[4]]\n",
    "        agentI = ipositions.pop(self.agentID)\n",
    "\n",
    "        jpositions = [positions[1], positions[3],  positions[5]]\n",
    "        agentJ = jpositions.pop(self.agentID)\n",
    "\n",
    "        otherAgentPositions = [[ipositions[0], jpositions[0]], [ipositions[1], jpositions[1]]]\n",
    "\n",
    "        agentBox = gamestate[self.agentID+6]\n",
    "\n",
    "        dropoff = gamestate[9:12]\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "\n",
    "        pickup = gamestate[12:]\n",
    "        pickupPositions = [[4,2], [3,3], [2,4]]\n",
    "\n",
    "        if(agentBox == 0):\n",
    "            legalMoves.remove(\"D\")\n",
    "            if([agentI, agentJ] not in pickupPositions):\n",
    "                legalMoves.remove(\"P\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in pickupPositions:\n",
    "                    if ([agentI, agentJ] == location and pickup[index] == 0):\n",
    "                        legalMoves.remove(\"P\")\n",
    "                    index += 1\n",
    "        else:\n",
    "            legalMoves.remove(\"P\")\n",
    "            if([agentI, agentJ] not in dropoffPositions):\n",
    "                legalMoves.remove(\"D\")\n",
    "            else:\n",
    "                index = 0\n",
    "                for location in dropoffPositions:\n",
    "                    if ([agentI, agentJ] == location and dropoff[index] == 5):\n",
    "                        legalMoves.remove(\"D\")\n",
    "                    index += 1\n",
    "\n",
    "        if([agentI-1, agentJ] in otherAgentPositions or agentI-1 == 0):\n",
    "            legalMoves.remove(\"N\")\n",
    "        if([agentI+1, agentJ] in otherAgentPositions or agentI+1 == 6):\n",
    "            legalMoves.remove(\"S\")\n",
    "        if([agentI, agentJ-1] in otherAgentPositions or agentJ-1 == 0):\n",
    "            legalMoves.remove(\"W\")   \n",
    "        if([agentI, agentJ+1] in otherAgentPositions or agentJ+1 == 6):\n",
    "            legalMoves.remove(\"E\")   \n",
    "\n",
    "        return legalMoves\n",
    "    \n",
    "    # takes the nextmove and returns the corresponding gamestate as a list of integers\n",
    "    def getGameState(self, gamestate, nextMove):\n",
    "        agentI = gamestate[self.agentID*2]\n",
    "        agentJ = gamestate[self.agentID*2+1]\n",
    "\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "        pickupPositions = [[1,5], [2,4], [5,2]]\n",
    "\n",
    "        if(nextMove == \"D\"):\n",
    "            index = 0\n",
    "            for location in dropoffPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[9+index] = gamestate[9+index] + 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 0\n",
    "\n",
    "        if(nextMove == \"P\"):\n",
    "            index = 0\n",
    "            for location in pickupPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[12+index] = gamestate[12+index] - 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 1\n",
    "\n",
    "        if(nextMove == \"N\"):\n",
    "            gamestate[self.agentID*2] = agentI-1\n",
    "        if(nextMove == \"S\"):\n",
    "            gamestate[self.agentID*2] = agentI+1\n",
    "        if(nextMove == \"W\"):\n",
    "            gamestate[self.agentID*2+1] = agentJ-1\n",
    "        if(nextMove == \"E\"):\n",
    "            gamestate[self.agentID*2+1] = agentJ+1\n",
    "\n",
    "        return gamestate\n",
    "\n",
    "    # this uses pickup locations for experiment 4\n",
    "    def getGameState4(self, gamestate, nextMove):\n",
    "        agentI = gamestate[self.agentID*2]\n",
    "        agentJ = gamestate[self.agentID*2+1]\n",
    "\n",
    "        dropoffPositions = [[1,1], [3,1], [4,5]]\n",
    "        pickupPositions = [[4,2], [3,3], [2,4]]\n",
    "\n",
    "        if(nextMove == \"D\"):\n",
    "            index = 0\n",
    "            for location in dropoffPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[9+index] = gamestate[9+index] + 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 0\n",
    "\n",
    "        if(nextMove == \"P\"):\n",
    "            index = 0\n",
    "            for location in pickupPositions:\n",
    "                if([agentI, agentJ] == location):\n",
    "                    gamestate[12+index] = gamestate[12+index] - 1\n",
    "                index += 1\n",
    "            gamestate[6+self.agentID] = 1\n",
    "\n",
    "        if(nextMove == \"N\"):\n",
    "            gamestate[self.agentID*2] = agentI-1\n",
    "        if(nextMove == \"S\"):\n",
    "            gamestate[self.agentID*2] = agentI+1\n",
    "        if(nextMove == \"W\"):\n",
    "            gamestate[self.agentID*2+1] = agentJ-1\n",
    "        if(nextMove == \"E\"):\n",
    "            gamestate[self.agentID*2+1] = agentJ+1\n",
    "\n",
    "        return gamestate\n",
    "\n",
    "\n",
    "    def stepS(self, gamestate, policy):\n",
    "        learning_method = \"S\"\n",
    "\n",
    "        next_move = self.getAgentNextStep()\n",
    "        position = self.getPosition(gamestate)\n",
    "        \n",
    "        if(next_move == \"\"):\n",
    "            legal_moves = self.getLegalMoves(gamestate)\n",
    "            if(\"P\" in legal_moves):\n",
    "                next_move = \"P\"\n",
    "            elif(\"D\" in legal_moves):\n",
    "                next_move = \"D\"\n",
    "            else:\n",
    "                next_move = random.choice(legal_moves)\n",
    "\n",
    "        next_position = position\n",
    "        next_gamestate = self.getGameState(gamestate, next_move)\n",
    "        next_legal_moves = self.getLegalMoves(next_gamestate)\n",
    "\n",
    "        if(next_move == \"P\"):\n",
    "            self.setPickupReward()\n",
    "            turn_reward = 13\n",
    "        elif(next_move == \"D\"):\n",
    "            self.setDropoffReward()\n",
    "            turn_reward = 13\n",
    "        else:\n",
    "            self.setMovePenalty()\n",
    "            next_position = self.getNextPosition(position, next_move)\n",
    "            next_gamestate = self.getGameState(gamestate, next_move)\n",
    "            next_legal_moves = self.getLegalMoves(next_gamestate)\n",
    "            turn_reward = -1\n",
    "\n",
    "        agent_next_step = self.qtable.get_best_move(next_position, next_legal_moves)    \n",
    "        self.setAgentNextStep(agent_next_step)\n",
    "\n",
    "        self.qtable.calc_q_value(position, next_move, turn_reward, next_position, next_legal_moves, agent_next_step, learning_method)\n",
    "        self.setRewardList(self.getReward())\n",
    "        \n",
    "        return next_gamestate\n",
    "\n",
    "    #learning_method is a string_arg passed on as SARSA or empty\n",
    "    def stepQ(self, gamestate, policy): \n",
    "        learning_method = \"Q\"\n",
    "        # Get legal moves for the agent\n",
    "        legal_moves = self.getLegalMoves(gamestate)\n",
    "        position = self.getPosition(gamestate)\n",
    "        \n",
    "        # Select action based on the Q-table and policy, and set agent reward accordingly\n",
    "        if \"P\" in legal_moves:\n",
    "            next_move = \"P\"\n",
    "            self.setPickupReward()\n",
    "            turn_reward=13\n",
    "        elif \"D\" in legal_moves:\n",
    "            next_move = \"D\"\n",
    "            self.setDropoffReward()\n",
    "            turn_reward=13\n",
    "        else: \n",
    "            # If neither pickup nor dropoff is available, use Q-table-based policy\n",
    "            next_move = self.qtable.select_action(position, policy, legal_moves)\n",
    "            self.setMovePenalty()\n",
    "            turn_reward=-1\n",
    "        \n",
    "        next_position = self.getNextPosition(position, next_move)\n",
    "        next_gamestate = self.getGameState(gamestate, next_move)\n",
    "        next_legal_moves = self.getLegalMoves(next_gamestate)\n",
    "\n",
    "        self.qtable.calc_q_value(position, next_move, turn_reward, next_position, next_legal_moves, \"NA\", learning_method)\n",
    "        self.setRewardList(self.getReward())\n",
    "        \n",
    "        return next_gamestate\n",
    "    \n",
    "    # this is for pickup locations in experiment 4\n",
    "    def stepQ4(self, gamestate, policy): \n",
    "        learning_method = \"Q\"\n",
    "        # Get legal moves for the agent\n",
    "        legal_moves = self.getLegalMoves4(gamestate)\n",
    "        position = self.getPosition(gamestate)\n",
    "        \n",
    "        # Select action based on the Q-table and policy, and set agent reward accordingly\n",
    "        if \"P\" in legal_moves:\n",
    "            next_move = \"P\"\n",
    "            self.setPickupReward()\n",
    "            turn_reward=13\n",
    "        elif \"D\" in legal_moves:\n",
    "            next_move = \"D\"\n",
    "            self.setDropoffReward()\n",
    "            turn_reward=13\n",
    "        else: \n",
    "            # If neither pickup nor dropoff is available, use Q-table-based policy\n",
    "            next_move = self.qtable.select_action(position, policy, legal_moves)\n",
    "            self.setMovePenalty()\n",
    "            turn_reward=-1\n",
    "        \n",
    "        next_position = self.getNextPosition(position, next_move)\n",
    "        next_gamestate = self.getGameState4(gamestate, next_move)\n",
    "        next_legal_moves = self.getLegalMoves4(next_gamestate)\n",
    "\n",
    "        self.qtable.calc_q_value(position, next_move, turn_reward, next_position, next_legal_moves, \"NA\", learning_method)\n",
    "        self.setRewardList(self.getReward())\n",
    "        \n",
    "        return next_gamestate\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishCheck(gamestate):\n",
    "    if(gamestate[9:] == [5,5,5,0,0,0]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tIn Experiment 1 you use learning rate=0.3 and discount factor=0.5, and run the traditional Q-learning algorithm for 9000 steps; initially you run the policy PRANDOM for 500 steps, then\n",
    "* \tContinue running PRANDOM for 8500 more steps  (only policy will change, agents will keep the behavior from the first training)\n",
    "*\tRun PGREEDY for the remaining 8500 steps (only policy will change, agents will keep the behavior from the first training)\n",
    "*\tRun PEXPLOIT for the remaining 8500 steps (only policy will change, agents will keep the behavior from the first training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am going to run the first 500 steps of prandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Random Run: [1, 2, 4, 4, 5, 5, 1, 0, 0, 2, 5, 5, 1, 1, 0]\n",
      "Red Reward: -68\n",
      "Blue Reward: -54\n",
      "Black Reward: -26\n",
      "Q-table:\n",
      "State   |     P       D      N      E      S      W\n",
      "------------------------------------------------------\n",
      "(0,0)   |    0.00   3.90  -0.66   0.00   0.00   0.00\n",
      "(0,1)   |    0.00   0.00   0.00  -0.30   0.00  -0.66\n",
      "(0,2)   |    0.00   0.00  -0.83  -0.30   0.00  -0.30\n",
      "(0,3)   |    0.00   0.00  -0.66  -0.76   0.00  -0.30\n",
      "(0,4)   |    3.90   0.00   0.00  -0.51   0.00   0.00\n",
      "(1,0)   |    0.00   0.00  -0.30   0.00  -0.51  -0.76\n",
      "(1,1)   |    0.00   0.00   0.00  -0.51  -0.66   0.00\n",
      "(1,2)   |    0.00   0.00  -0.55  -0.30  -0.30  -0.55\n",
      "(1,3)   |    6.67   0.00  -0.30   0.28  -0.80  -0.47\n",
      "(1,4)   |    0.00   0.00   0.00  -0.51  -0.30   0.00\n",
      "(2,0)   |    0.00   8.71   0.10   0.00  -0.51  -0.30\n",
      "(2,1)   |    0.00   0.00  -0.66  -0.51   0.00  -0.51\n",
      "(2,2)   |    0.00   0.00  -0.76  -0.51   0.00  -0.51\n",
      "(2,3)   |    0.00   0.00   0.00  -0.30  -0.51  -0.30\n",
      "(2,4)   |    0.00   0.00   0.00   0.00  -0.30   0.00\n",
      "(3,0)   |    0.00   0.00  -0.30   0.00  -0.76  -0.83\n",
      "(3,1)   |    0.00   0.00  -0.51  -0.91  -0.93  -0.55\n",
      "(3,2)   |    0.00   0.00  -0.73  -0.51  -0.73  -0.55\n",
      "(3,3)   |    0.00   0.00  -0.55  -0.55  -0.30  -0.30\n",
      "(3,4)   |    0.00   3.90  -0.51   0.00   0.00   0.00\n",
      "(4,0)   |    0.00   0.00   0.00   0.00  -0.51  -0.66\n",
      "(4,1)   |    6.63   0.00   0.00  -0.76   0.07  -0.51\n",
      "(4,2)   |    0.00   0.00   0.00  -0.73  -0.51  -0.76\n",
      "(4,3)   |    0.00   0.00   0.00  -0.76  -0.76  -0.30\n",
      "(4,4)   |    0.00   0.00   0.00  -0.66  -0.30   0.00\n",
      "\n",
      "Second Run: [2, 5, 3, 3, 2, 1, 0, 1, 0, 0, 0, 0, 5, 5, 4]\n",
      "Red Reward: -1893\n",
      "Blue Reward: -2061\n",
      "Black Reward: -1739\n",
      "\n",
      "Greedy Run: [2, 4, 5, 4, 2, 3, 1, 0, 0, 0, 0, 0, 5, 4, 5]\n",
      "Red Reward: -4712\n",
      "Blue Reward: -4894\n",
      "Black Reward: -4572\n",
      "\n",
      "Exploit Run: [3, 1, 3, 4, 2, 2, 0, 1, 0, 2, 3, 5, 4, 0, 0]\n",
      "Red Reward: -7321\n",
      "Blue Reward: -7461\n",
      "Black Reward: -7181\n"
     ]
    }
   ],
   "source": [
    "environment_rows=5\n",
    "environment_cols=5\n",
    "num_actions = 6\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.5\n",
    "\n",
    "redAgent = Agent(0)\n",
    "redAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "blueAgent = Agent(1)\n",
    "blueAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "blackAgent = Agent(2)\n",
    "blackAgent.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "\n",
    "initialEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "testEnvironment =    [1,2,3,4,5,6,7,8,9,0,1,2,3,4,5]\n",
    "currEnvironment = initialEnvironment\n",
    "policy = \"PRANDOM\"\n",
    "\n",
    "first500 = int(500/3)\n",
    "for i in range(first500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "print(f\"First Random Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "blackAgent.qtable.print_q_table()\n",
    "\n",
    "\n",
    "# Second Round\n",
    "currEnvironment = None\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "random8500 = int(8500/3)\n",
    "for i in range(random8500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "print(f\"\\nSecond Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n",
    "\n",
    "# Third Round\n",
    "currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "greedy8500 = int(8500/3)\n",
    "policy = \"PGREEDY\"\n",
    "for i in range(greedy8500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "print(f\"\\nGreedy Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")\n",
    "\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit8500 = int(8500/3)\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "print(f\"\\nExploit Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent.getReward()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 \n",
    "* 1.c except *run SARSA* q-learning variation for 9000 steps\n",
    "\n",
    "When analyzing Experiment 2 \n",
    "* Comparing the performance of Q-learning and SARSA\n",
    "* Report one of the final Q-tables of this experiment\n",
    "* Also assess the quality of agent coordination, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_rows=5\n",
    "environment_cols=5\n",
    "num_actions = 6\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.5\n",
    "\n",
    "redAgent2 = Agent(0)\n",
    "redAgent2.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate, discount_factor)\n",
    "\n",
    "blueAgent2 = Agent(1)\n",
    "blueAgent2.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "blackAgent2 = Agent(2)\n",
    "blackAgent2.setAgentQTable(environment_rows, environment_cols, num_actions, learning_rate,discount_factor)\n",
    "\n",
    "currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit9000 = int(9000/3)\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit9000):\n",
    "    currEnvironment = redAgent2.stepS(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent2.stepS(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent2.stepS(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "print(f\"\\nGreedy Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent2.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent2.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent2.getReward()}\")\n",
    "print(\"Black Agent QTable\")\n",
    "blackAgent2.qtable.print_q_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 \n",
    "rerun either  Experiment 1.c or 2 with learning rates:\n",
    "* 0.15 \n",
    "* 0.45\n",
    "\n",
    "When interpreting the results focus on analyzing the effects of using the 3 different learning rates on the system performance. \n",
    "##### We will be rerunning experiment 1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploit Run: [3, 2, 3, 4, 4, 2, 0, 0, 1, 3, 5, 5, 1, 0, 0]\n",
      "Red Reward: -2777\n",
      "Blue Reward: -2637\n",
      "Black Reward: -2287\n",
      "Black Agent QTable\n",
      "Q-table:\n",
      "State    |   N       E       S       W\n",
      "----------------------------------------\n",
      "(0,0)   |  0.00   6.21   -0.33  0.00   0.00   -0.21 \n",
      "(0,1)   |  0.00   0.00   -1.00  -0.56  0.00   -0.78 \n",
      "(0,2)   |  0.00   0.00   -1.00  -0.94  0.00   -0.92 \n",
      "(0,3)   |  0.00   0.00   -1.00  -0.90  0.00   -0.62 \n",
      "(0,4)   |  7.23   0.00   -0.46  -0.51  0.00   0.00  \n",
      "(1,0)   |  0.00   0.00   -0.50  0.00   -0.62  0.00  \n",
      "(1,1)   |  0.00   0.00   -0.29  -0.59  -1.13  -0.63 \n",
      "(1,2)   |  0.00   0.00   -0.66  -0.29  -1.13  -1.14 \n",
      "(1,3)   |  7.23   0.00   -0.28  -1.01  -1.14  -0.91 \n",
      "(1,4)   |  0.00   0.00   -0.15  -0.73  -0.98  0.00  \n",
      "(2,0)   |  0.00   10.44  -0.69  0.00   0.00   -0.38 \n",
      "(2,1)   |  0.00   0.00   -1.05  -0.40  -0.49  -1.14 \n",
      "(2,2)   |  0.00   0.00   -1.04  -1.10  -0.42  -0.41 \n",
      "(2,3)   |  0.00   0.00   -0.89  -0.15  -0.84  -0.51 \n",
      "(2,4)   |  0.00   0.00   -0.86  -0.97  0.00   0.00  \n",
      "(3,0)   |  0.00   0.00   -1.01  0.00   -1.00  -1.00 \n",
      "(3,1)   |  0.00   0.00   -1.20  -1.20  -0.41  -1.20 \n",
      "(3,2)   |  0.00   0.00   -1.52  -1.46  -1.17  -1.08 \n",
      "(3,3)   |  0.00   0.00   -1.07  -0.90  -0.15  -1.08 \n",
      "(3,4)   |  0.00   7.23   -0.98  -0.90  -0.53  0.00  \n",
      "(4,0)   |  0.00   0.00   0.00   0.00   -0.91  -0.99 \n",
      "(4,1)   |  10.44  0.00   0.00   -0.48  -1.00  -0.86 \n",
      "(4,2)   |  0.00   0.00   0.00   -1.00  -1.00  -0.88 \n",
      "(4,3)   |  0.00   0.00   0.00   -0.62  -1.00  -0.56 \n",
      "(4,4)   |  0.00   0.00   0.00   -0.88  -0.91  0.00  \n",
      "\n",
      "Exploit Run: [3, 4, 2, 2, 3, 2, 1, 0, 1, 1, 3, 5, 0, 0, 4]\n",
      "Red Reward: -2455\n",
      "Blue Reward: -2497\n",
      "Black Reward: -2427\n",
      "Black Agent QTable\n",
      "Q-table:\n",
      "State    |   N       E       S       W\n",
      "----------------------------------------\n",
      "(0,0)   |  0.00   9.07   -0.95  0.00   0.00   -0.98 \n",
      "(0,1)   |  0.00   0.00   -1.00  -0.91  0.00   -1.00 \n",
      "(0,2)   |  0.00   0.00   -1.00  -1.00  0.00   -1.35 \n",
      "(0,3)   |  0.00   0.00   -1.00  -1.12  0.00   -0.98 \n",
      "(0,4)   |  12.80  0.00   -0.99  3.09   0.00   0.00  \n",
      "(1,0)   |  0.00   0.00   -1.00  0.00   -1.00  -0.98 \n",
      "(1,1)   |  0.00   0.00   -1.57  -1.64  -1.77  -1.60 \n",
      "(1,2)   |  0.00   0.00   -1.80  -1.69  -1.87  -1.80 \n",
      "(1,3)   |  14.63  0.00   0.95   -1.22  -0.53  -0.52 \n",
      "(1,4)   |  0.00   0.00   -1.00  -1.00  -1.00  0.00  \n",
      "(2,0)   |  0.00   12.35  0.00   0.00   -1.00  3.11  \n",
      "(2,1)   |  0.00   0.00   -1.10  -0.45  -0.95  -1.22 \n",
      "(2,2)   |  0.00   0.00   -1.87  -1.89  -1.89  -1.81 \n",
      "(2,3)   |  0.00   0.00   -1.75  -1.73  -1.51  -1.74 \n",
      "(2,4)   |  0.00   0.00   -1.00  -1.00  -1.00  0.00  \n",
      "(3,0)   |  0.00   0.00   -0.45  0.00   -0.70  0.00  \n",
      "(3,1)   |  0.00   0.00   -0.61  -0.70  -1.27  -1.25 \n",
      "(3,2)   |  0.00   0.00   -1.36  -1.07  -1.51  -1.53 \n",
      "(3,3)   |  0.00   0.00   -1.86  -1.92  -1.93  -1.91 \n",
      "(3,4)   |  0.00   12.80  -1.00  -1.00  -0.45  0.00  \n",
      "(4,0)   |  0.00   0.00   0.00   0.00   -0.45  0.00  \n",
      "(4,1)   |  0.00   0.00   0.00   0.00   -0.45  0.00  \n",
      "(4,2)   |  0.00   0.00   0.00   0.00   -0.91  -0.45 \n",
      "(4,3)   |  0.00   0.00   0.00   0.00   -1.00  -0.91 \n",
      "(4,4)   |  0.00   0.00   0.00   -0.70  -1.00  0.00  \n"
     ]
    }
   ],
   "source": [
    "environment_rows=5\n",
    "environment_cols=5\n",
    "num_actions = 6\n",
    "a1 = 0.15         # learning rate 1\n",
    "a2 = 0.45         # learning rate 2\n",
    "discount_factor = 0.5\n",
    "\n",
    "redAgent3a1 = Agent(0)\n",
    "redAgent3a1.setAgentQTable(environment_rows, environment_cols, num_actions, a1, discount_factor)\n",
    "\n",
    "blueAgent3a1 = Agent(1)\n",
    "blueAgent3a1.setAgentQTable(environment_rows, environment_cols, num_actions, a1, discount_factor)\n",
    "\n",
    "blackAgent3a1 = Agent(2)\n",
    "blackAgent3a1.setAgentQTable(environment_rows, environment_cols, num_actions, a1, discount_factor)\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit8500 = int(8500/3)\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent3a1.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent3a1.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent3a1.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "print(f\"\\nExploit Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent3a1.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent3a1.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent3a1.getReward()}\")\n",
    "\n",
    "print(\"Black Agent QTable\")\n",
    "blackAgent3a1.qtable.print_q_table()\n",
    "\n",
    "redAgent3a2 = Agent(0)\n",
    "redAgent3a2.setAgentQTable(environment_rows, environment_cols, num_actions, a2, discount_factor)\n",
    "\n",
    "blueAgent3a2 = Agent(1)\n",
    "blueAgent3a2.setAgentQTable(environment_rows, environment_cols, num_actions, a2, discount_factor)\n",
    "\n",
    "blackAgent3a2 = Agent(2)\n",
    "blackAgent3a2.setAgentQTable(environment_rows, environment_cols, num_actions, a2, discount_factor)\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit8500 = int(8500/3)\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent3a2.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent3a2.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent3a2.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "print(f\"\\nExploit Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent3a2.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent3a2.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent3a2.getReward()}\")\n",
    "\n",
    "print(\"Black Agent QTable\")\n",
    "blackAgent3a2.qtable.print_q_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 \n",
    "is the somewhat similar to Experiment 1c use:\n",
    "* a=0.3 \n",
    "* a=0.5 \n",
    "\n",
    "in conjunction with either Q-learning or SARSA  as follows: \n",
    "1. run PRANDOM for the first 500 steps\n",
    "2. run PEXPLOIT \n",
    "3. when, a terminal state is reached the third time change the three pickup locations to: (4,2), (3,3) and (2,4)\n",
    "4. run PEXPLOIT with the “new” pickup locations until the agent reaches a terminal state the sixth time. \n",
    "\n",
    "When interpreting the results of this experiment center on analyzing on how well the learning strategy was able to adapt to the change of the pickup locations and to which extend it was able to learn “new” paths and unlearn “old” paths which became obsolete.\n",
    "\n",
    "##### We Will be using Experiment 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploit Run: [3, 3, 5, 3, 1, 3, 0, 0, 0, 0, 0, 0, 5, 5, 5]\n",
      "Red Reward: -4040\n",
      "Blue Reward: -4068\n",
      "Black Reward: -3830\n",
      "Black Agent QTable\n",
      "Q-table:\n",
      "State    |   N       E       S       W\n",
      "----------------------------------------\n",
      "(0,0)   |  0.00   6.21   -0.33  0.00   0.00   -0.21 \n",
      "(0,1)   |  0.00   0.00   -1.00  -0.56  0.00   -0.78 \n",
      "(0,2)   |  0.00   0.00   -1.00  -0.94  0.00   -0.92 \n",
      "(0,3)   |  0.00   0.00   -1.00  -0.90  0.00   -0.62 \n",
      "(0,4)   |  7.23   0.00   -0.46  -0.51  0.00   0.00  \n",
      "(1,0)   |  0.00   0.00   -0.50  0.00   -0.62  0.00  \n",
      "(1,1)   |  0.00   0.00   -0.29  -0.59  -1.13  -0.63 \n",
      "(1,2)   |  0.00   0.00   -0.66  -0.29  -1.13  -1.14 \n",
      "(1,3)   |  7.23   0.00   -0.28  -1.01  -1.14  -0.91 \n",
      "(1,4)   |  0.00   0.00   -0.15  -0.73  -0.98  0.00  \n",
      "(2,0)   |  0.00   10.44  -0.69  0.00   0.00   -0.38 \n",
      "(2,1)   |  0.00   0.00   -1.05  -0.40  -0.49  -1.14 \n",
      "(2,2)   |  0.00   0.00   -1.04  -1.10  -0.42  -0.41 \n",
      "(2,3)   |  0.00   0.00   -0.89  -0.15  -0.84  -0.51 \n",
      "(2,4)   |  0.00   0.00   -0.86  -0.97  0.00   0.00  \n",
      "(3,0)   |  0.00   0.00   -1.01  0.00   -1.00  -1.00 \n",
      "(3,1)   |  0.00   0.00   -1.20  -1.20  -0.41  -1.20 \n",
      "(3,2)   |  0.00   0.00   -1.52  -1.46  -1.17  -1.08 \n",
      "(3,3)   |  0.00   0.00   -1.07  -0.90  -0.15  -1.08 \n",
      "(3,4)   |  0.00   7.23   -0.98  -0.90  -0.53  0.00  \n",
      "(4,0)   |  0.00   0.00   0.00   0.00   -0.91  -0.99 \n",
      "(4,1)   |  10.44  0.00   0.00   -0.48  -1.00  -0.86 \n",
      "(4,2)   |  0.00   0.00   0.00   -1.00  -1.00  -0.88 \n",
      "(4,3)   |  0.00   0.00   0.00   -0.62  -1.00  -0.56 \n",
      "(4,4)   |  0.00   0.00   0.00   -0.88  -0.91  0.00  \n",
      "\n",
      "Exploit Run: [2, 4, 5, 3, 4, 4, 1, 1, 1, 0, 5, 5, 0, 0, 2]\n",
      "Red Reward: -5174\n",
      "Blue Reward: -5300\n",
      "Black Reward: -5160\n",
      "Black Agent QTable\n",
      "Q-table:\n",
      "State    |   N       E       S       W\n",
      "----------------------------------------\n",
      "(0,0)   |  0.00   12.89  0.74   0.00   0.00   -0.97 \n",
      "(0,1)   |  0.00   0.00   -1.00  -1.19  0.00   -1.31 \n",
      "(0,2)   |  0.00   0.00   -1.00  -0.83  0.00   -1.00 \n",
      "(0,3)   |  0.00   0.00   -1.00  -1.04  0.00   -1.07 \n",
      "(0,4)   |  10.84  0.00   -1.00  -0.99  0.00   0.00  \n",
      "(1,0)   |  0.00   0.00   -1.02  0.00   -1.04  -1.00 \n",
      "(1,1)   |  0.00   0.00   -1.98  -1.98  -1.96  -1.97 \n",
      "(1,2)   |  0.00   0.00   -1.95  -1.91  -1.94  -1.95 \n",
      "(1,3)   |  14.01  0.00   -0.53  -0.22  -0.71  0.76  \n",
      "(1,4)   |  0.00   0.00   -1.12  -1.00  -1.00  0.00  \n",
      "(2,0)   |  0.00   14.32  0.74   0.00   3.84   2.68  \n",
      "(2,1)   |  0.00   0.00   -1.99  -1.99  -1.99  -1.99 \n",
      "(2,2)   |  13.57  0.00   0.78   0.54   0.13   1.95  \n",
      "(2,3)   |  0.00   0.00   -1.66  -1.70  -1.53  -1.70 \n",
      "(2,4)   |  0.00   0.00   -1.00  -1.00  -1.00  0.00  \n",
      "(3,0)   |  0.00   0.00   -1.00  0.00   -1.30  -1.00 \n",
      "(3,1)   |  14.10  0.00   -0.94  3.53   -1.16  -1.53 \n",
      "(3,2)   |  0.00   0.00   -1.66  -1.74  -1.82  -1.81 \n",
      "(3,3)   |  0.00   0.00   -1.61  -1.26  -1.63  -1.63 \n",
      "(3,4)   |  0.00   13.15  -0.77  -1.00  -0.38  0.00  \n",
      "(4,0)   |  0.00   0.00   0.00   0.00   -1.00  -1.00 \n",
      "(4,1)   |  5.85   0.00   0.00   -1.00  -1.00  -0.98 \n",
      "(4,2)   |  0.00   0.00   0.00   -1.04  -0.95  -0.45 \n",
      "(4,3)   |  0.00   0.00   0.00   -0.70  -1.00  -0.98 \n",
      "(4,4)   |  0.00   0.00   0.00   -0.83  -1.00  0.00  \n"
     ]
    }
   ],
   "source": [
    "environment_rows=5\n",
    "environment_cols=5\n",
    "num_actions = 6\n",
    "a1 = 0.15         # learning rate 1\n",
    "a2 = 0.45         # learning rate 2\n",
    "discount_factor = 0.5\n",
    "\n",
    "# TEST WITH ALPHA 1\n",
    "redAgent4a1 = Agent(0)\n",
    "redAgent4a1.setAgentQTable(environment_rows, environment_cols, num_actions, a1, discount_factor)\n",
    "\n",
    "blueAgent4a1 = Agent(1)\n",
    "blueAgent4a1.setAgentQTable(environment_rows, environment_cols, num_actions, a1, discount_factor)\n",
    "\n",
    "blackAgent4a1 = Agent(2)\n",
    "blackAgent4a1.setAgentQTable(environment_rows, environment_cols, num_actions, a1, discount_factor)\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "random500 = int(500/3)\n",
    "policy = \"PRANDOM\"\n",
    "for i in range(random500):\n",
    "    currEnvironment = redAgent4a1.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent4a1.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent4a1.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit8500 = int(8500/3)\n",
    "finishCount = 0\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent4a1.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent4a1.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent4a1.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        finishCount += 1\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "        if(finishCount == 3):\n",
    "            break\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit8500 = int(8500/3)\n",
    "finishCount = 0\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent4a1.stepQ4(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent4a1.stepQ4(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent4a1.stepQ4(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        finishCount += 1\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "        if(finishCount == 3):\n",
    "            break\n",
    "\n",
    "print(f\"\\nExploit Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent4a1.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent4a1.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent4a1.getReward()}\")\n",
    "\n",
    "print(\"Black Agent QTable\")\n",
    "blackAgent3a1.qtable.print_q_table()\n",
    "\n",
    "\n",
    "# TEST WITH ALPHA 2\n",
    "redAgent4a2 = Agent(0)\n",
    "redAgent4a2.setAgentQTable(environment_rows, environment_cols, num_actions, a2, discount_factor)\n",
    "\n",
    "blueAgent4a2 = Agent(1)\n",
    "blueAgent4a2.setAgentQTable(environment_rows, environment_cols, num_actions, a2, discount_factor)\n",
    "\n",
    "blackAgent4a2 = Agent(2)\n",
    "blackAgent4a2.setAgentQTable(environment_rows, environment_cols, num_actions, a2, discount_factor)\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "random500 = int(500/3)\n",
    "policy = \"PRANDOM\"\n",
    "for i in range(random500):\n",
    "    currEnvironment = redAgent4a2.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent4a2.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent4a2.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit8500 = int(8500/3)\n",
    "finishCount = 0\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent4a2.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent4a2.stepQ(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent4a2.stepQ(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        finishCount += 1\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "        if(finishCount == 3):\n",
    "            break\n",
    "\n",
    "currEnvironment = [3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "exploit8500 = int(8500/3)\n",
    "finishCount = 0\n",
    "policy = \"PEXPLOIT\"\n",
    "for i in range(exploit8500):\n",
    "    currEnvironment = redAgent4a2.stepQ4(currEnvironment, policy)\n",
    "    currEnvironment = blueAgent4a2.stepQ4(currEnvironment, policy)\n",
    "    currEnvironment = blackAgent4a2.stepQ4(currEnvironment, policy)\n",
    "    if(finishCheck(currEnvironment)):\n",
    "        finishCount += 1\n",
    "        currEnvironment=[3,3,5,3,1,3,0,0,0,0,0,0,5,5,5]\n",
    "        if(finishCount == 3):\n",
    "            break\n",
    "\n",
    "print(f\"\\nExploit Run: {currEnvironment}\")\n",
    "print(f\"Red Reward: {redAgent4a2.getReward()}\")\n",
    "print(f\"Blue Reward: {blueAgent4a2.getReward()}\")\n",
    "print(f\"Black Reward: {blackAgent4a2.getReward()}\")\n",
    "\n",
    "print(\"Black Agent QTable\")\n",
    "blackAgent4a2.qtable.print_q_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
